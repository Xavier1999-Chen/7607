{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEd-mtF0PTrv"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to Moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8c9YBduQCI4"
      },
      "source": [
        "## 1 $n$-gram Language Model\n",
        "**Q1**: Expand the above definition of $ p(\\vec{w})$ using naive estimates of the parameters, such as $  p(w_4 \\mid w_2, w_3) \\stackrel{\\tiny{\\mbox{def}}}{=}  \\frac{C(w_2~w_3~w_4)}{C(w_2~w_3)} $ where \\( C(w_2 w_3 w_4) \\) denotes the count of times the trigram $ w_2 w_3 w_4 $ was observed in a training corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMQ_Z1g8QZef"
      },
      "source": [
        "**Write your answer:**\n",
        "\n",
        "\\# todo\n",
        "\n",
        "$  p(\\vec{w}) \\stackrel{\\tiny{\\mbox{def}}}{=}\n",
        "\\frac{C(w_1)}{|\\vec{w}|} \\cdot\n",
        "\\frac{C(w_1~w_2)}{C(w_1)} \\cdot\n",
        "\\frac{C(w_1~w_2~w_3)}{C(w_1~w_2)} \\cdot\n",
        "\\frac{C(w_2~w_3~w_4)}{C(w_2~w_3)} \\\\ \\cdots\n",
        "\\frac{C(w_n-2~w_n-1~w_n)}{C(w_n-1~w_n)}  $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmgExbf1QtCH"
      },
      "source": [
        "**Q2**: One could also define a kind of reversed trigram language model $p_{reversed}$ that instead assumed the words were generated in reverse order (from right to left):\n",
        "\\begin{align} p_{reversed}(\\vec{w}) \\stackrel{\\tiny{\\mbox{def}}}{=}&p(w_n) \\cdot p(w_{n-1} \\mid w_n) \\cdot p(w_{n-2} \\mid w_{n-1} w_n) \\cdot p(w_{n-3} \\mid w_{n-2} w_{n-1}) \\\\ &\\cdots p(w_2 \\mid w_3 w_4) \\cdot p(w_1 \\mid w_2 w_3) \\end{align}\n",
        "By manipulating the notation, show that the two models are identical, i.e., $ p(\\vec{w}) = p_{reversed}(\\vec{w}) $ for any $ \\vec{w} $ provided that both models use MLE parameters estimated from the same training data (see Q1 above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm1ZGFIaRPCP"
      },
      "source": [
        "**Write your answer:**\n",
        "\n",
        "\\# todo\n",
        "\n",
        "$  p(\\vec{w}) \\stackrel{\\tiny{\\mbox{def}}}{=}\n",
        "\\frac{C(w_1)}{|\\vec{w}|} \\cdot\n",
        "\\frac{C(w_1~w_2)}{C(w_1)} \\cdot\n",
        "\\frac{C(w_1~w_2~w_3)}{C(w_1~w_2)} \\cdot\n",
        "\\frac{C(w_2~w_3~w_4)}{C(w_2~w_3)} \\\\ \\cdots\n",
        "\\frac{C(w_n-2~w_n-1~w_n)}{C(w_n-2~w_n-1)}\n",
        "\\\\ {=}\n",
        "\\frac{C(w_n)}{|\\vec{w}|} \\cdot\n",
        "\\frac{C(w_n~w_n-1)}{C(w_n)} \\cdot\n",
        "\\frac{C(w_n~w_n-1~w_n-2)}{C(w_n~w_n-1)} \\cdot\n",
        "\\frac{C(w_n-1~w_n-2~w_n-3)}{C(w_n-1~w_n-2)} \\\\ \\cdots\n",
        "\\frac{C(w_3~w_2~w_1)}{C(w_3~w_2)}  $\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQEc5kz4RniG"
      },
      "source": [
        "## 2 $N$-gram Language Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3kSwtN79jWgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caac9b61-db07-4b41-b19b-19f283008702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-20 21:19:01--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6640478 (6.3M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]   6.33M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-10-20 21:19:02 (94.7 MB/s) - ‘train.txt’ saved [6640478/6640478]\n",
            "\n",
            "--2023-10-20 21:19:02--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 872910 (852K) [text/plain]\n",
            "Saving to: ‘dev.txt’\n",
            "\n",
            "dev.txt             100%[===================>] 852.45K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-10-20 21:19:02 (25.9 MB/s) - ‘dev.txt’ saved [872910/872910]\n",
            "\n",
            "--2023-10-20 21:19:02--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 869318 (849K) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>] 848.94K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-10-20 21:19:03 (20.9 MB/s) - ‘test.txt’ saved [869318/869318]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O train.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/train.txt\n",
        "!wget -O dev.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/dev.txt\n",
        "!wget -O test.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9HCVQwqkTc_"
      },
      "source": [
        "### 2.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KhFKCzwkaTn"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI_sEK7J8265",
        "outputId": "217e4dbd-399d-4fcf-8a21-63d37c708e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import gmean\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk import lm\n",
        "from nltk.lm import preprocessing\n",
        "import io\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.util import ngrams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp4XO27R8265",
        "outputId": "00ae28e8-17bf-44a7-c21e-189994fd6939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size is: 16948\n"
          ]
        }
      ],
      "source": [
        "train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "sents = train.sents()\n",
        "# words = train.words()\n",
        "\n",
        "\n",
        "# Add start-of-sentence tokens <s> and end-of-sentence </s> tokens.\n",
        "uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "tri_train,v = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "# Convert tokens that occur less than three times in the training data into a special unknown token <UNK>.\n",
        "vocab = lm.Vocabulary(v,unk_cutoff=3)\n",
        "\n",
        "# Print the vocabulary size.\n",
        "print(\"Vocabulary size is:\", len(vocab))\n",
        "\n",
        "# for sent in train_text:\n",
        "#     print(list(sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OUA2OxK8265"
      },
      "outputs": [],
      "source": [
        "# from functools import partial\n",
        "# def padded_ngram_pipeline(order, text):\n",
        "#     padding_fn = partial(preprocessing.pad_both_ends, n=order)\n",
        "#     return (\n",
        "#         (ngrams(list(padding_fn(sent)), n=order) for sent in text),\n",
        "#         preprocessing.flatten(map(padding_fn, text)),\n",
        "    # )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "i-QRRTgR8265"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLQNsUA5kfZe"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n",
        "There are 1 parameter in unigram model, 2 in bigram model, 3 in trigram model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJzDNMVikkeX"
      },
      "source": [
        "### 2.2 $N$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxkcs2HykuR2"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aC8wjUT38266"
      },
      "outputs": [],
      "source": [
        "uni_model = lm.MLE(1)\n",
        "bi_model  = lm.MLE(2)\n",
        "tri_model = lm.MLE(3)\n",
        "\n",
        "uni_model.fit(uni_train,vocab)\n",
        "bi_model.fit(bi_train,vocab)\n",
        "# tri_model.fit(tri_train,vocab)\n",
        "\n",
        "# len(bi_model.vocab)\n",
        "# bi_model.vocab.lookup(\"</s>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmoQbyRw8266",
        "outputId": "20b0695c-b23b-46be-dfa0-57d6a783d61e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Unigram Model over train dataset: 977.7254663002035\n",
            "Perplexity of Bigram Model over train dataset: 216.62822602581886\n"
          ]
        }
      ],
      "source": [
        "train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "sents = train.sents()\n",
        "\n",
        "uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "# tri_train,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "ppl = []\n",
        "for sent in uni_train:\n",
        "    ppl.append(uni_model.perplexity(list(sent)))\n",
        "ppl_mean = gmean(ppl)\n",
        "print(\"Perplexity of Unigram Model over train dataset:\",ppl_mean)\n",
        "\n",
        "ppl = []\n",
        "for sent in bi_train:\n",
        "    ppl.append(bi_model.perplexity(list(sent)))\n",
        "ppl_mean = gmean(ppl)\n",
        "print(\"Perplexity of Bigram Model over train dataset:\",ppl_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ub3blke8266",
        "outputId": "7553ec46-b0eb-4761-f470-1a6c8c6d2920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Unigram Model over dev dataset: 945.1187134309084\n",
            "Perplexity of Bigram Model over dev dataset: inf\n"
          ]
        }
      ],
      "source": [
        "test  = PlaintextCorpusReader(\"./\",\"dev.txt\")\n",
        "sents = test.sents()\n",
        "\n",
        "uni_dev,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_dev,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "tri_dev,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "ppl = []\n",
        "for sent in uni_dev:\n",
        "    ppl.append(uni_model.perplexity(list(sent)))\n",
        "ppl_mean = gmean(ppl)\n",
        "print(\"Perplexity of Unigram Model over dev dataset:\",ppl_mean)\n",
        "\n",
        "ppl = []\n",
        "for sent in bi_dev:\n",
        "    ppl.append(bi_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(\"Perplexity of Bigram Model over dev dataset:\",ppl_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3o9Nez8kvYm"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOQUqM73kzf-"
      },
      "source": [
        "### 2.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LgXRmJwk3Y-"
      },
      "source": [
        "#### 2.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFG7jCIRk7Qw"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LqOyCHJy8267"
      },
      "outputs": [],
      "source": [
        "train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "sents = train.sents()\n",
        "\n",
        "# uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "# tri_train,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "# uni_Laplace_model = lm.Laplace(1)\n",
        "bi_Laplace_model  = lm.Laplace(2)\n",
        "# tri_Laplace_model = lm.Laplace(3)\n",
        "\n",
        "# uni_Laplace_model.fit(uni_train,vocab)\n",
        "bi_Laplace_model.fit(bi_train,vocab)\n",
        "# tri_Laplace_model.fit(tri_train,vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkycUOnd8267",
        "outputId": "3109ff9a-4b4f-4001-8306-d3e0ca880331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Bigram Model over train dataset: 799.5663817970321\n"
          ]
        }
      ],
      "source": [
        "train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "sents = train.sents()\n",
        "\n",
        "# uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "# tri_train,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "ppl = []\n",
        "for sent in bi_train:\n",
        "    ppl.append(bi_Laplace_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(\"Perplexity of Bigram Model over train dataset:\",ppl_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehF2YwXf8267",
        "outputId": "b7871d06-b0fe-413a-abef-32322e138842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Bigram Model over dev dataset: 885.955985029454\n"
          ]
        }
      ],
      "source": [
        "test  = PlaintextCorpusReader(\"./\",\"dev.txt\")\n",
        "sents = test.sents()\n",
        "\n",
        "# uni_dev,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_dev,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "# tri_dev,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "ppl = []\n",
        "for sent in bi_dev:\n",
        "    ppl.append(bi_Laplace_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(\"Perplexity of Bigram Model over dev dataset:\",ppl_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36yTKPXFk8f2"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8cFbczqlBR_"
      },
      "source": [
        "#### 2.3.2: Add-$k$ smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_ZiAgIlPUu"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJLz5nCa8268",
        "outputId": "b881cd50-0e11-4b9b-e9b9-f299192b1be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Bigram Model over dev dataset when gamma is 0.5: 758.5314311570586\n"
          ]
        }
      ],
      "source": [
        "train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "sents = train.sents()\n",
        "\n",
        "# uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "# tri_train,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "g = 0.5\n",
        "\n",
        "# uni_Lidstone_model = lm.Lidstone(g,1,vocabulary=vocab)\n",
        "bi_Lidstone_model  = lm.Lidstone(g,2,vocabulary=vocab)\n",
        "# tri_Lidstone_model = lm.Lidstone(g,3,vocabulary=vocab)\n",
        "\n",
        "# uni_Lidstone_model.fit(uni_train)\n",
        "bi_Lidstone_model.fit(bi_train)\n",
        "# tri_Lidstone_model.fit(tri_train)\n",
        "\n",
        "test  = PlaintextCorpusReader(\"./\",\"dev.txt\")\n",
        "sents = test.sents()\n",
        "\n",
        "# uni_dev,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_dev,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "# tri_dev,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "ppl = []\n",
        "for sent in bi_dev:\n",
        "    ppl.append(bi_Lidstone_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(f\"Perplexity of Bigram Model over dev dataset when gamma is {g}:\",ppl_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKV92ISd8268",
        "outputId": "e4258a32-4dde-4071-d7b6-74beef2ea76e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Bigram Model over dev dataset when gamma is 0.05: 512.8467604417024\n"
          ]
        }
      ],
      "source": [
        "train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "sents = train.sents()\n",
        "\n",
        "# uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "# tri_train,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "g = 0.05\n",
        "\n",
        "# uni_Lidstone_model = lm.Lidstone(g,1,vocabulary=vocab)\n",
        "bi_Lidstone_model  = lm.Lidstone(g,2,vocabulary=vocab)\n",
        "# tri_Lidstone_model = lm.Lidstone(g,3,vocabulary=vocab)\n",
        "\n",
        "# uni_Lidstone_model.fit(uni_train)\n",
        "bi_Lidstone_model.fit(bi_train)\n",
        "# tri_Lidstone_model.fit(tri_train)\n",
        "\n",
        "test  = PlaintextCorpusReader(\"./\",\"dev.txt\")\n",
        "sents = test.sents()\n",
        "\n",
        "# uni_dev,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_dev,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "# tri_dev,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "ppl = []\n",
        "for sent in bi_dev:\n",
        "    ppl.append(bi_Lidstone_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(f\"Perplexity of Bigram Model over dev dataset when gamma is {g}:\",ppl_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnJHUa1K8268",
        "outputId": "9d61146d-b949-47f5-eaae-0701621a86d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Bigram Model over dev dataset when gamma is 0.01: 449.62736817084533\n"
          ]
        }
      ],
      "source": [
        "train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "sents = train.sents()\n",
        "\n",
        "# uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "# tri_train,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "g = 0.01\n",
        "\n",
        "# uni_Lidstone_model = lm.Lidstone(g,1,vocabulary=vocab)\n",
        "bi_Lidstone_model  = lm.Lidstone(g,2,vocabulary=vocab)\n",
        "# tri_Lidstone_model = lm.Lidstone(g,3,vocabulary=vocab)\n",
        "\n",
        "# uni_Lidstone_model.fit(uni_train)\n",
        "bi_Lidstone_model.fit(bi_train)\n",
        "# tri_Lidstone_model.fit(tri_train)\n",
        "\n",
        "test  = PlaintextCorpusReader(\"./\",\"dev.txt\")\n",
        "sents = test.sents()\n",
        "\n",
        "# uni_dev,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_dev,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "# tri_dev,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "ppl = []\n",
        "for sent in bi_dev:\n",
        "    ppl.append(bi_Lidstone_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(f\"Perplexity of Bigram Model over dev dataset when gamma is {g}:\",ppl_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHFNf8OIlQ0O"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjKEO_TqlUrX"
      },
      "source": [
        "#### 2.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcdd4cvYlZuO"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BO-mi8eg8268"
      },
      "outputs": [],
      "source": [
        "train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "sents = train.sents()\n",
        "\n",
        "uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "tri_train,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "a = 0.4\n",
        "\n",
        "uni_StupidBackoff_model = lm.StupidBackoff(alpha=a,order=1,vocabulary=vocab)\n",
        "bi_StupidBackoff_model  = lm.StupidBackoff(alpha=a,order=2,vocabulary=vocab)\n",
        "tri_StupidBackoff_model = lm.StupidBackoff(alpha=a,order=3,vocabulary=vocab)\n",
        "\n",
        "uni_StupidBackoff_model.fit(uni_train)\n",
        "bi_StupidBackoff_model.fit(bi_train)\n",
        "tri_StupidBackoff_model.fit(tri_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctl-cVBJ8269",
        "outputId": "70199821-2c83-42cb-a734-f0ddd37dd206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Unigram Model over train dataset when alpha is 0.4: 977.7254663002035\n",
            "Perplexity of Bigram Model over train dataset when alpha is 0.4: 216.62822602581886\n",
            "Perplexity of Trigram Model over train dataset when alpha is 0.4: 57.53778400254985\n"
          ]
        }
      ],
      "source": [
        "train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "sents = train.sents()\n",
        "uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "tri_train,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "# Unigram\n",
        "ppl = []\n",
        "for sent in uni_train:\n",
        "    ppl.append(uni_StupidBackoff_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(f\"Perplexity of Unigram Model over train dataset when alpha is {a}:\",ppl_mean)\n",
        "\n",
        "# Bigram\n",
        "ppl = []\n",
        "for sent in bi_train:\n",
        "    ppl.append(bi_StupidBackoff_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(f\"Perplexity of Bigram Model over train dataset when alpha is {a}:\",ppl_mean)\n",
        "\n",
        "# Trigram\n",
        "ppl = []\n",
        "for sent in tri_train:\n",
        "    ppl.append(tri_StupidBackoff_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(f\"Perplexity of Trigram Model over train dataset when alpha is {a}:\",ppl_mean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFsB7bUX8269",
        "outputId": "c0f61491-006a-41a7-842c-c075a1b65df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Unigram Model over dev dataset when alpha is 0.4: 945.1187134309084\n",
            "Perplexity of Bigram Model over dev dataset when alpha is 0.4: 338.8204045373383\n",
            "Perplexity of Trigram Model over dev dataset when alpha is 0.4: 205.61093312112462\n"
          ]
        }
      ],
      "source": [
        "test  = PlaintextCorpusReader(\"./\",\"dev.txt\")\n",
        "sents = test.sents()\n",
        "uni_dev,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "bi_dev,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "tri_dev,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "# Unigram\n",
        "ppl = []\n",
        "for sent in uni_dev:\n",
        "    ppl.append(uni_StupidBackoff_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(f\"Perplexity of Unigram Model over dev dataset when alpha is {a}:\",ppl_mean)\n",
        "\n",
        "# Bigram\n",
        "ppl = []\n",
        "for sent in bi_dev:\n",
        "    ppl.append(bi_StupidBackoff_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(f\"Perplexity of Bigram Model over dev dataset when alpha is {a}:\",ppl_mean)\n",
        "\n",
        "# Trigram\n",
        "ppl = []\n",
        "for sent in tri_dev:\n",
        "    ppl.append(tri_StupidBackoff_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(f\"Perplexity of Trigram Model over dev dataset when alpha is {a}:\",ppl_mean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTiyzHbQ8269",
        "outputId": "827c0f72-0da5-4953-84e9-0764c4d76b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Unigram Model over dev dataset when alpha is 0.1: 945.1187134309084\n",
            "Perplexity of Unigram Model over dev dataset when alpha is 0.3: 945.1187134309084\n",
            "Perplexity of Unigram Model over dev dataset when alpha is 0.5: 945.1187134309084\n",
            "Perplexity of Unigram Model over dev dataset when alpha is 0.7: 945.1187134309084\n",
            "Perplexity of Unigram Model over dev dataset when alpha is 0.9: 945.1187134309084\n"
          ]
        }
      ],
      "source": [
        "for aa in range(1,10,2):\n",
        "    a = aa/10\n",
        "    train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "    sents = train.sents()\n",
        "\n",
        "    uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "    # bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "    # tri_train,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "    uni_StupidBackoff_model = lm.StupidBackoff(alpha=a,order=1,vocabulary=vocab)\n",
        "    # bi_StupidBackoff_model  = lm.StupidBackoff(alpha=a,order=2,vocabulary=vocab)\n",
        "    # tri_StupidBackoff_model = lm.StupidBackoff(alpha=a,order=3,vocabulary=vocab)\n",
        "\n",
        "    uni_StupidBackoff_model.fit(uni_train)\n",
        "    # bi_StupidBackoff_model.fit(bi_train)\n",
        "    # tri_StupidBackoff_model.fit(tri_train)\n",
        "\n",
        "    test  = PlaintextCorpusReader(\"./\",\"dev.txt\")\n",
        "    sents = test.sents()\n",
        "    uni_dev,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "    # bi_dev,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "    # tri_dev,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "    ppl = []\n",
        "    for sent in uni_dev:\n",
        "        ppl.append(uni_StupidBackoff_model.perplexity(list(sent)))\n",
        "    # ppl_mean = np.mean(ppl)\n",
        "    ppl_mean = gmean(ppl)\n",
        "    print(f\"Perplexity of Unigram Model over dev dataset when alpha is {a}:\",ppl_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-qpfIZy827A",
        "outputId": "e8e2e8ca-47f0-41c8-92ee-048707de0233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Bigram Model over dev dataset when alpha is 0.1: 384.1621036225339\n",
            "Perplexity of Bigram Model over dev dataset when alpha is 0.3: 347.7672326137989\n",
            "Perplexity of Bigram Model over dev dataset when alpha is 0.5: 332.0395208219873\n",
            "Perplexity of Bigram Model over dev dataset when alpha is 0.7: 322.0705270560666\n",
            "Perplexity of Bigram Model over dev dataset when alpha is 0.9: 314.82035041825503\n"
          ]
        }
      ],
      "source": [
        "for aa in range(1,10,2):\n",
        "    a = aa/10\n",
        "    train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "    sents = train.sents()\n",
        "\n",
        "    # uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "    bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "    # tri_train,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "    # uni_StupidBackoff_model = lm.StupidBackoff(alpha=a,order=1,vocabulary=vocab)\n",
        "    bi_StupidBackoff_model  = lm.StupidBackoff(alpha=a,order=2,vocabulary=vocab)\n",
        "    # tri_StupidBackoff_model = lm.StupidBackoff(alpha=a,order=3,vocabulary=vocab)\n",
        "\n",
        "    # uni_StupidBackoff_model.fit(uni_train)\n",
        "    bi_StupidBackoff_model.fit(bi_train)\n",
        "    # tri_StupidBackoff_model.fit(tri_train)\n",
        "\n",
        "    test  = PlaintextCorpusReader(\"./\",\"dev.txt\")\n",
        "    sents = test.sents()\n",
        "    # uni_dev,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "    bi_dev,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "    # tri_dev,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "    ppl = []\n",
        "    for sent in bi_dev:\n",
        "        ppl.append(bi_StupidBackoff_model.perplexity(list(sent)))\n",
        "    # ppl_mean = np.mean(ppl)\n",
        "    ppl_mean = gmean(ppl)\n",
        "    print(f\"Perplexity of Bigram Model over dev dataset when alpha is {a}:\",ppl_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oplYtQhS827A",
        "outputId": "7f0227cf-8593-4f0e-eceb-9ac928f11596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Trigram Model over dev dataset when alpha is 0.1: 301.7857806636453\n",
            "Perplexity of Trigram Model over dev dataset when alpha is 0.3: 222.65360786315298\n",
            "Perplexity of Trigram Model over dev dataset when alpha is 0.5: 193.29522779487237\n",
            "Perplexity of Trigram Model over dev dataset when alpha is 0.7: 176.10525602129067\n",
            "Perplexity of Trigram Model over dev dataset when alpha is 0.9: 164.2709241815869\n"
          ]
        }
      ],
      "source": [
        "for aa in range(1,10,2):\n",
        "    a = aa/10\n",
        "    train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "    sents = train.sents()\n",
        "\n",
        "    # uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "    # bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "    tri_train,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "    # uni_StupidBackoff_model = lm.StupidBackoff(alpha=a,order=1,vocabulary=vocab)\n",
        "    # bi_StupidBackoff_model  = lm.StupidBackoff(alpha=a,order=2,vocabulary=vocab)\n",
        "    tri_StupidBackoff_model = lm.StupidBackoff(alpha=a,order=3,vocabulary=vocab)\n",
        "\n",
        "    # uni_StupidBackoff_model.fit(uni_train)\n",
        "    # bi_StupidBackoff_model.fit(bi_train)\n",
        "    tri_StupidBackoff_model.fit(tri_train)\n",
        "\n",
        "    test  = PlaintextCorpusReader(\"./\",\"dev.txt\")\n",
        "    sents = test.sents()\n",
        "    # uni_dev,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "    # bi_dev,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "    tri_dev,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "    ppl = []\n",
        "    for sent in tri_dev:\n",
        "        ppl.append(tri_StupidBackoff_model.perplexity(list(sent)))\n",
        "    # ppl_mean = np.mean(ppl)\n",
        "    ppl_mean = gmean(ppl)\n",
        "    print(f\"Perplexity of Trigram Model over dev dataset when alpha is {a}:\",ppl_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aZsquw9827A",
        "outputId": "dc7f6bb0-76e6-4628-bf89-4e84fdf465b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of Trigram Model over test dataset when alpha is 0.9: 161.4663188121758\n"
          ]
        }
      ],
      "source": [
        "train = PlaintextCorpusReader(\"./\",\"train.txt\")\n",
        "sents = train.sents()\n",
        "\n",
        "# uni_train,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "# bi_train,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "tri_train,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "a = 0.9\n",
        "\n",
        "# uni_StupidBackoff_model = lm.StupidBackoff(alpha=a,order=1,vocabulary=vocab)\n",
        "# bi_StupidBackoff_model  = lm.StupidBackoff(alpha=a,order=2,vocabulary=vocab)\n",
        "tri_StupidBackoff_model = lm.StupidBackoff(alpha=a,order=3,vocabulary=vocab)\n",
        "\n",
        "# uni_StupidBackoff_model.fit(uni_train)\n",
        "# bi_StupidBackoff_model.fit(bi_train)\n",
        "tri_StupidBackoff_model.fit(tri_train)\n",
        "\n",
        "test  = PlaintextCorpusReader(\"./\",\"test.txt\")\n",
        "sents = test.sents()\n",
        "# uni_test,_ = preprocessing.padded_everygram_pipeline(1,sents)\n",
        "# bi_test,_  = preprocessing.padded_everygram_pipeline(2,sents)\n",
        "tri_test,_ = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "ppl = []\n",
        "for sent in tri_test:\n",
        "    ppl.append(tri_StupidBackoff_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(f\"Perplexity of Trigram Model over test dataset when alpha is {a}:\",ppl_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyKqmQ37lcH2"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzSbk2bClf3u"
      },
      "source": [
        "##### **Optimization**:\n",
        "\n",
        "\\# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgTcTlLuloHu"
      },
      "source": [
        "## 3 Preposition Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "7jb0OQ-yltc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4023be-6123-480a-ad74-2ed7af86c3f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-20 21:37:57--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.in\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 210427 (205K) [text/plain]\n",
            "Saving to: ‘dev.in’\n",
            "\n",
            "dev.in              100%[===================>] 205.50K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-10-20 21:37:57 (10.1 MB/s) - ‘dev.in’ saved [210427/210427]\n",
            "\n",
            "--2023-10-20 21:37:57--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.out\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10018 (9.8K) [text/plain]\n",
            "Saving to: ‘dev.out’\n",
            "\n",
            "dev.out             100%[===================>]   9.78K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-10-20 21:37:57 (103 MB/s) - ‘dev.out’ saved [10018/10018]\n",
            "\n",
            "--2023-10-20 21:37:57--  https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/test.in\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68304 (67K) [text/plain]\n",
            "Saving to: ‘test.in’\n",
            "\n",
            "test.in             100%[===================>]  66.70K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-10-20 21:37:58 (4.40 MB/s) - ‘test.in’ saved [68304/68304]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O dev.in https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.in\n",
        "!wget -O dev.out https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.out\n",
        "!wget -O test.in https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/test.in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4HjiWLw827A",
        "outputId": "493a8be9-357d-4d4d-974d-90e95e09c81d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at of this at \n",
            "\n",
            "of \n",
            "\n",
            "of \n",
            "\n",
            "destroyed of on \n",
            "\n",
            "of in for on \n",
            "\n",
            "is for \n",
            "\n",
            "on \n",
            "\n",
            "of in in of of \n",
            "\n",
            "on of to \n",
            "\n",
            "of \n",
            "\n",
            "in \n",
            "\n",
            "of of of , \n",
            "\n",
            "in \n",
            "\n",
            "in for \n",
            "\n",
            "of in \n",
            "\n",
            "in of \n",
            "\n",
            "available of \n",
            "\n",
            ". of \n",
            "\n",
            "in at of \n",
            "\n",
            "of to at at <UNK> \n",
            "\n",
            "in in of \n",
            "\n",
            "in of of . \n",
            "\n",
            "into for of \n",
            "\n",
            "on in for of for \n",
            "\n",
            "at in of \n",
            "\n",
            "level \n",
            "\n",
            "at \n",
            "\n",
            "in in at of \n",
            "\n",
            "adulyadej in . \n",
            "\n",
            "of \n",
            "\n",
            "on for for \n",
            "\n",
            "being of \n",
            "\n",
            "on , \n",
            "\n",
            "so of \n",
            "\n",
            "in of \n",
            "\n",
            "of of \n",
            "\n",
            "in of \n",
            "\n",
            "outside \n",
            "\n",
            "in in \n",
            "\n",
            "in of in of \n",
            "\n",
            "on symptoms in \n",
            "\n",
            "of in \n",
            "\n",
            "on older in of \n",
            "\n",
            "in . friendly \n",
            "\n",
            "has \n",
            "\n",
            ", of to of \n",
            "\n",
            "on in financial of \n",
            "\n",
            "for \n",
            "\n",
            "of \n",
            "\n",
            "of of \n",
            "\n",
            "being of \n",
            "\n",
            "on \n",
            "\n",
            "of in with in it \n",
            "\n",
            "over of of of on \n",
            "\n",
            "in on \n",
            "\n",
            "in in \n",
            "\n",
            "in \n",
            "\n",
            "at for \n",
            "\n",
            "entered of in of \n",
            "\n",
            "in markets on \n",
            "\n",
            "in by \n",
            "\n",
            "in in in \n",
            "\n",
            "on in in \n",
            "\n",
            "of for will \n",
            "\n",
            "for in in \n",
            "\n",
            "in on for of \n",
            "\n",
            ", \n",
            "\n",
            "tuesday \n",
            "\n",
            "in \n",
            "\n",
            "in \n",
            "\n",
            "early \n",
            "\n",
            "on on \n",
            "\n",
            "of \n",
            "\n",
            "attacked of on reviewing \n",
            "\n",
            "on for in for \n",
            "\n",
            "on in on for \n",
            "\n",
            "in in \n",
            "\n",
            "on on \n",
            "\n",
            "of \n",
            "\n",
            "near sensor in on in \n",
            "\n",
            "searching - in on in on of at \n",
            "\n",
            "of for at for \n",
            "\n",
            "on of \n",
            "\n",
            "on of , \n",
            "\n",
            "of of in \n",
            "\n",
            "on of with of \n",
            "\n",
            "in of on \n",
            "\n",
            "of of \n",
            "\n",
            "of as \n",
            "\n",
            "in at of in \n",
            "\n",
            "for for \n",
            "\n",
            "of of here \n",
            "\n",
            "in \n",
            "\n",
            "in email \n",
            "\n",
            "in - \n",
            "\n",
            "on agency of \n",
            "\n",
            "on \n",
            "\n",
            "of \n",
            "\n",
            "this for and \n",
            "\n",
            "on at \n",
            "\n",
            "of \n",
            "\n",
            "in of for \n",
            "\n",
            "for \n",
            "\n",
            "in in in in of of \n",
            "\n",
            "on of on \n",
            "\n",
            "in in of \n",
            "\n",
            "- or \n",
            "\n",
            "of of \n",
            "\n",
            "tuesday for \n",
            "\n",
            "of in of of at \n",
            "\n",
            "at donald at of for \n",
            "\n",
            "in \n",
            "\n",
            "in \n",
            "\n",
            "of of in of . \n",
            "\n",
            "for \n",
            "\n",
            "of in at \n",
            "\n",
            "and on \n",
            "\n",
            "of of \n",
            "\n",
            "at \n",
            "\n",
            "on of \n",
            "\n",
            "of for \n",
            "\n",
            "of \n",
            "\n",
            ", \n",
            "\n",
            "of rover of conducted \n",
            "\n",
            "on \n",
            "\n",
            "for \n",
            "\n",
            "of in \n",
            "\n",
            "at \n",
            "\n",
            "on in \n",
            "\n",
            "of of of for \n",
            "\n",
            "of \n",
            "\n",
            "in drive of in of of \n",
            "\n",
            "in field in system of \n",
            "\n",
            "of \n",
            "\n",
            "in \n",
            "\n",
            "in \n",
            "\n",
            "at \n",
            "\n",
            "in of in \n",
            "\n",
            "on \n",
            "\n",
            "of of on of in \n",
            "\n",
            "to \n",
            "\n",
            "of on \n",
            "\n",
            "for \n",
            "\n",
            "on of in at \n",
            "\n",
            "in on \n",
            "\n",
            "in on \n",
            "\n",
            "wednesday \n",
            "\n",
            "at \n",
            "\n",
            ". on \n",
            "\n",
            "in \n",
            "\n",
            "in \n",
            "\n",
            "for in \n",
            "\n",
            "in sector of for \n",
            "\n",
            "of \n",
            "\n",
            "for , \n",
            "\n",
            "in at \n",
            "\n",
            "in in on of \n",
            "\n",
            "on ltd for ' \n",
            "\n",
            "of at in . \n",
            "\n",
            "in hosts on in in \n",
            "\n",
            "for of \n",
            "\n",
            "on in \n",
            "\n",
            "of in of \n",
            "\n",
            "on in of \n",
            "\n",
            "on in of \n",
            "\n",
            "in of , on of \n",
            "\n",
            "on with \n",
            "\n",
            "room in \n",
            "\n",
            "on in \n",
            "\n",
            "of for ' for against \n",
            "\n",
            ", \n",
            "\n",
            ", \n",
            "\n",
            "for in in \n",
            "\n",
            "in at of \n",
            "\n",
            "- \n",
            "\n",
            "in resort of on \n",
            "\n",
            ", \n",
            "\n",
            "in on of on on \n",
            "\n",
            "on at \n",
            "\n",
            "despite . of for of \n",
            "\n",
            "on for of \n",
            "\n",
            "that on \n",
            "\n",
            "for president \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "on \n",
            "\n",
            "on \n",
            "\n",
            "in trial of \n",
            "\n",
            "of \n",
            "\n",
            "of , \n",
            "\n",
            "of for of \n",
            "\n",
            "of \n",
            "\n",
            "on \n",
            "\n",
            "on '' for \n",
            "\n",
            "comes on \n",
            "\n",
            "to on in in of \n",
            "\n",
            "in who \n",
            "\n",
            "of for \n",
            "\n",
            "in of \n",
            "\n",
            "of in in for in \n",
            "\n",
            "of of for \n",
            "\n",
            "in in at \n",
            "\n",
            "here of on of \n",
            "\n",
            "in \n",
            "\n",
            "of on of \n",
            "\n",
            "due for of in of \n",
            "\n",
            "on in in \n",
            "\n",
            "in \n",
            "\n",
            "of , \n",
            "\n",
            "of in on \n",
            "\n",
            "of fatigue \n",
            "\n",
            "on on in of \n",
            "\n",
            "on thin of \n",
            "\n",
            "at on of of of \n",
            "\n",
            "in almost in \n",
            "\n",
            "for in on \n",
            "\n",
            "of on on \n",
            "\n",
            "of \n",
            "\n",
            "on of bus at \n",
            "\n",
            "said \n",
            "\n",
            "in in at of \n",
            "\n",
            "which \n",
            "\n",
            "for in of of a \n",
            "\n",
            ", early \n",
            "\n",
            "in for on for of \n",
            "\n",
            "in on \n",
            "\n",
            "in . \n",
            "\n",
            "on . \n",
            "\n",
            "on of in of \n",
            "\n",
            ", for \n",
            "\n",
            "in on in \n",
            "\n",
            "of in in \n",
            "\n",
            "in of on \n",
            "\n",
            "in of of \n",
            "\n",
            "of in in \n",
            "\n",
            "of on in in of in \n",
            "\n",
            ". on \n",
            "\n",
            "on in in of in \n",
            "\n",
            "of on in \n",
            "\n",
            "on \n",
            "\n",
            "of of in in \n",
            "\n",
            "reading \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "thursday segments \n",
            "\n",
            "of in in \n",
            "\n",
            "in in of most of \n",
            "\n",
            "on \n",
            "\n",
            "early \n",
            "\n",
            "in of \n",
            "\n",
            "on of in \n",
            "\n",
            "in in for for \n",
            "\n",
            "of from \n",
            "\n",
            "at of at \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "for \n",
            "\n",
            "of in wednesday \n",
            "\n",
            "on on on \n",
            "\n",
            "of for \n",
            "\n",
            "on \n",
            "\n",
            "for on \n",
            "\n",
            "said in in \n",
            "\n",
            "on on in on of \n",
            "\n",
            "said on \n",
            "\n",
            "at of \n",
            "\n",
            "of in \n",
            "\n",
            "of on detonation \n",
            "\n",
            "on \n",
            "\n",
            "in at \n",
            "\n",
            "of \n",
            "\n",
            "of \n",
            "\n",
            "on in of of \n",
            "\n",
            "in at \n",
            "\n",
            "of in \n",
            "\n",
            "nation \n",
            "\n",
            "on and \n",
            "\n",
            "of at \n",
            "\n",
            "of on in on \n",
            "\n",
            "of of of . of of \n",
            "\n",
            "at \n",
            "\n",
            "in \n",
            "\n",
            "in in of \n",
            "\n",
            "of of in \n",
            "\n",
            "in at \n",
            "\n",
            "a \n",
            "\n",
            "in for \n",
            "\n",
            "in takes \n",
            "\n",
            "in on \n",
            "\n",
            "at on of \n",
            "\n",
            "of \n",
            "\n",
            "on in in to \n",
            "\n",
            "in \n",
            "\n",
            "on in of \n",
            "\n",
            "'' in for \n",
            "\n",
            "of \n",
            "\n",
            "' \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "in of on in of \n",
            "\n",
            "<UNK> following on of \n",
            "\n",
            "on in \n",
            "\n",
            "of of in \n",
            "\n",
            "agreement of \n",
            "\n",
            "of for \n",
            "\n",
            "on in in in \n",
            "\n",
            "of which on \n",
            "\n",
            "to centred . \n",
            "\n",
            "in \n",
            "\n",
            "power of for \n",
            "\n",
            "<UNK> in \n",
            "\n",
            "of at in in \n",
            "\n",
            "at in of \n",
            "\n",
            "of \n",
            "\n",
            "on for commander for \n",
            "\n",
            "in of \n",
            "\n",
            ", \n",
            "\n",
            "the in \n",
            "\n",
            "of \n",
            "\n",
            "to in on in \n",
            "\n",
            "to in on \n",
            "\n",
            "how of in \n",
            "\n",
            "and in \n",
            "\n",
            "of and \n",
            "\n",
            "on \n",
            "\n",
            "in in in \n",
            "\n",
            "in for on \n",
            "\n",
            "for in \n",
            "\n",
            "on company \n",
            "\n",
            "on \n",
            "\n",
            "for of \n",
            "\n",
            "of in of for in \n",
            "\n",
            "in on \n",
            "\n",
            "for \n",
            "\n",
            "in of of \n",
            "\n",
            "tv of which for \n",
            "\n",
            "of for of \n",
            "\n",
            "of in \n",
            "\n",
            "of of for \n",
            "\n",
            "of \n",
            "\n",
            "even in on \n",
            "\n",
            "take at on on in of of \n",
            "\n",
            "of , \n",
            "\n",
            "to of of on \n",
            "\n",
            "in of \n",
            "\n",
            "at on \n",
            "\n",
            "to \n",
            "\n",
            "in on five for living of \n",
            "\n",
            "on \n",
            "\n",
            "of of for in in \n",
            "\n",
            ", , \n",
            "\n",
            "on of in in \n",
            "\n",
            "in \n",
            "\n",
            "after out \n",
            "\n",
            "over \n",
            "\n",
            "for at \n",
            "\n",
            ". setting \n",
            "\n",
            "in \n",
            "\n",
            "of . \n",
            "\n",
            "of at of \n",
            "\n",
            ", in \n",
            "\n",
            "chain \n",
            "\n",
            "at in ' \n",
            "\n",
            "in \n",
            "\n",
            "ahead in at be can \n",
            "\n",
            "of in \n",
            "\n",
            "showed in \n",
            "\n",
            "on on at \n",
            "\n",
            "city \n",
            "\n",
            ", \n",
            "\n",
            "at on at \n",
            "\n",
            "of and \n",
            "\n",
            "of on of \n",
            "\n",
            "for in \n",
            "\n",
            "a \n",
            "\n",
            "of of at \n",
            "\n",
            "of \n",
            "\n",
            "in \n",
            "\n",
            "in for of \n",
            "\n",
            "of for \n",
            "\n",
            "on of will \n",
            "\n",
            "their of source \n",
            "\n",
            "in on of for \n",
            "\n",
            "at \n",
            "\n",
            "on of for \n",
            "\n",
            "collaborating on wednesday \n",
            "\n",
            "of \n",
            "\n",
            "after - of \n",
            "\n",
            "in \n",
            "\n",
            "in \n",
            "\n",
            "in \n",
            "\n",
            "of to in \n",
            "\n",
            "of in \n",
            "\n",
            "of groups of \n",
            "\n",
            "of on in \n",
            "\n",
            "of on of \n",
            "\n",
            "of on at for \n",
            "\n",
            "on after of \n",
            "\n",
            "of of of to for \n",
            "\n",
            "of from in on \n",
            "\n",
            "at in of for \n",
            "\n",
            "of in in \n",
            "\n",
            "of <UNK> of and of in for \n",
            "\n",
            "in on of \n",
            "\n",
            "of . in of of \n",
            "\n",
            "in \n",
            "\n",
            "of were on \n",
            "\n",
            "in of \n",
            "\n",
            "wednesday after of \n",
            "\n",
            "on on \n",
            "\n",
            "of in of of \n",
            "\n",
            "and when \n",
            "\n",
            ", \n",
            "\n",
            "of in of \n",
            "\n",
            "on of \n",
            "\n",
            "of programs to \n",
            "\n",
            "of canberra of in of \n",
            "\n",
            "in of \n",
            "\n",
            "in . of \n",
            "\n",
            "by in \n",
            "\n",
            "of to \n",
            "\n",
            "in on \n",
            "\n",
            "following a for over of \n",
            "\n",
            "in of \n",
            "\n",
            "has of of \n",
            "\n",
            "at \n",
            "\n",
            "components in \n",
            "\n",
            "next in of \n",
            "\n",
            "or to on \n",
            "\n",
            "on of \n",
            "\n",
            "on \n",
            "\n",
            "in for of \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "in of \n",
            "\n",
            "of \n",
            "\n",
            "on of \n",
            "\n",
            "of \n",
            "\n",
            "in \n",
            "\n",
            "for strongly on \n",
            "\n",
            "of of for \n",
            "\n",
            "of , in of \n",
            "\n",
            "by on on \n",
            "\n",
            "of of of older \n",
            "\n",
            "in one \n",
            "\n",
            "on over \n",
            "\n",
            "in \n",
            "\n",
            "of in of \n",
            "\n",
            "at on of of \n",
            "\n",
            "of to of older \n",
            "\n",
            "for \n",
            "\n",
            "of \n",
            "\n",
            "on at \n",
            "\n",
            "sunday on on \n",
            "\n",
            "for and updates on of of \n",
            "\n",
            "to \n",
            "\n",
            "upside of of \n",
            "\n",
            "thing in on of \n",
            "\n",
            "of of \n",
            "\n",
            "for of for \n",
            "\n",
            "on in of \n",
            "\n",
            "of for in . \n",
            "\n",
            "for in on \n",
            "\n",
            "for and updates on of of \n",
            "\n",
            "on of on \n",
            "\n",
            "in in for \n",
            "\n",
            "after for on \n",
            "\n",
            "on of of \n",
            "\n",
            "on in on of \n",
            "\n",
            "in nations on \n",
            "\n",
            "attack in of \n",
            "\n",
            "in of of \n",
            "\n",
            "in in in on \n",
            "\n",
            "for of on \n",
            "\n",
            "of on \n",
            "\n",
            "of friday of \n",
            "\n",
            "of in in \n",
            "\n",
            "moscow of winners here of , \n",
            "\n",
            "on for of \n",
            "\n",
            ", on for \n",
            "\n",
            "at of \n",
            "\n",
            "in in \n",
            "\n",
            "of for in \n",
            "\n",
            "of of in \n",
            "\n",
            ". \n",
            "\n",
            "for in at for \n",
            "\n",
            "for talks of for of \n",
            "\n",
            "in in in \n",
            "\n",
            "in at of on \n",
            "\n",
            "in \n",
            "\n",
            "on in gathered \n",
            "\n",
            "on of in in of older \n",
            "\n",
            "of in of of \n",
            "\n",
            "of . of \n",
            "\n",
            "of in \n",
            "\n",
            "ben of in \n",
            "\n",
            "on on of \n",
            "\n",
            "friday \n",
            "\n",
            "ben of at \n",
            "\n",
            "in forward \n",
            "\n",
            "al on \n",
            "\n",
            "of \n",
            "\n",
            "on \n",
            "\n",
            "of in \n",
            "\n",
            "in at in \n",
            "\n",
            "of at \n",
            "\n",
            "in \n",
            "\n",
            "on of for \n",
            "\n",
            "in of \n",
            "\n",
            "for of \n",
            "\n",
            "of morning in for \n",
            "\n",
            "for of in on \n",
            "\n",
            "for in \n",
            "\n",
            "in for of on \n",
            "\n",
            "wall of of \n",
            "\n",
            ", on \n",
            "\n",
            ". of \n",
            "\n",
            "of in on \n",
            "\n",
            "in \n",
            "\n",
            "for in ' \n",
            "\n",
            "- \n",
            "\n",
            "for month \n",
            "\n",
            "of for of \n",
            "\n",
            "in of \n",
            "\n",
            "at \n",
            "\n",
            "by \n",
            "\n",
            "of at for in \n",
            "\n",
            "in of of weeks \n",
            "\n",
            "tuesday at \n",
            "\n",
            "of in of on \n",
            "\n",
            "for in \n",
            "\n",
            "fleeing in \n",
            "\n",
            "and \n",
            "\n",
            "at year \n",
            "\n",
            "of , of \n",
            "\n",
            "for yasser . \n",
            "\n",
            "for for \n",
            "\n",
            "at on for of \n",
            "\n",
            "into \n",
            "\n",
            "tiles in \n",
            "\n",
            "of in on in of \n",
            "\n",
            "of of . \n",
            "\n",
            "of \n",
            "\n",
            "said of in \n",
            "\n",
            "of on at in of \n",
            "\n",
            "pro on . giant of \n",
            "\n",
            "for \n",
            "\n",
            "on of of of \n",
            "\n",
            "in for , of sunday \n",
            "\n",
            "of of \n",
            "\n",
            "of on of in in \n",
            "\n",
            "in in \n",
            "\n",
            "the to \n",
            "\n",
            ". \n",
            "\n",
            "for on of \n",
            "\n",
            "that in of \n",
            "\n",
            "has officials \n",
            "\n",
            "for on \n",
            "\n",
            "' on in removed at in \n",
            "\n",
            "in for \n",
            "\n",
            "of in \n",
            "\n",
            "in in in \n",
            "\n",
            "and of in \n",
            "\n",
            "in \n",
            "\n",
            "in of of \n",
            "\n",
            "in in \n",
            "\n",
            "for \n",
            "\n",
            "has will \n",
            "\n",
            "in \n",
            "\n",
            ". of \n",
            "\n",
            "on \n",
            "\n",
            "of in of in \n",
            "\n",
            "in \n",
            "\n",
            "in of and of \n",
            "\n",
            "in in \n",
            "\n",
            "in the of \n",
            "\n",
            "on of movement \n",
            "\n",
            "for of \n",
            "\n",
            "custody in - of \n",
            "\n",
            "of \n",
            "\n",
            "on of \n",
            "\n",
            "in \n",
            "\n",
            "in for that \n",
            "\n",
            "of in \n",
            "\n",
            "of \n",
            "\n",
            "of \n",
            "\n",
            "on in \n",
            "\n",
            "in \n",
            "\n",
            "about \n",
            "\n",
            "in broadcast of of \n",
            "\n",
            "in in in in \n",
            "\n",
            "cause for of , \n",
            "\n",
            "of on and \n",
            "\n",
            "and of a in \n",
            "\n",
            "as on \n",
            "\n",
            "in of \n",
            "\n",
            "on \n",
            "\n",
            "in in \n",
            "\n",
            "in for in \n",
            "\n",
            "on on \n",
            "\n",
            "of \n",
            "\n",
            "for of \n",
            "\n",
            "in for in \n",
            "\n",
            "on of \n",
            "\n",
            "in of \n",
            "\n",
            "of in \n",
            "\n",
            "of who \n",
            "\n",
            "to in in \n",
            "\n",
            "said \n",
            "\n",
            "on on \n",
            "\n",
            ", older of \n",
            "\n",
            "speaker against for legend \n",
            "\n",
            ", of \n",
            "\n",
            "of \n",
            "\n",
            "of \n",
            "\n",
            "of for humans for \n",
            "\n",
            "in \n",
            "\n",
            "more of - in in \n",
            "\n",
            "of for \n",
            "\n",
            "for at \n",
            "\n",
            "headache of of in \n",
            "\n",
            "of \n",
            "\n",
            "for in \n",
            "\n",
            "of at \n",
            "\n",
            "gave of \n",
            "\n",
            "of \n",
            "\n",
            "here \n",
            "\n",
            "of in for \n",
            "\n",
            "deserted in of \n",
            "\n",
            "for on \n",
            "\n",
            "for \n",
            "\n",
            ". of of \n",
            "\n",
            "in \n",
            "\n",
            "for in in in \n",
            "\n",
            "of on \n",
            "\n",
            "on of on at on \n",
            "\n",
            "in \n",
            "\n",
            "on in in of \n",
            "\n",
            "of for \n",
            "\n",
            "on on \n",
            "\n",
            "demonstrated in \n",
            "\n",
            "in \n",
            "\n",
            "of of \n",
            "\n",
            "in \n",
            "\n",
            "on \n",
            "\n",
            "of \n",
            "\n",
            "in her in on in \n",
            "\n",
            "on away of \n",
            "\n",
            "with for of \n",
            "\n",
            "in in of in \n",
            "\n",
            "on of \n",
            "\n",
            "on \n",
            "\n",
            "of \n",
            "\n",
            "in \n",
            "\n",
            "on in \n",
            "\n",
            "in of who of \n",
            "\n",
            "' \n",
            "\n",
            "monday \n",
            "\n",
            "on away of \n",
            "\n",
            "of \n",
            "\n",
            "on \n",
            "\n",
            "of alleging \n",
            "\n",
            "of in \n",
            "\n",
            "nine for \n",
            "\n",
            "revolt in in at on \n",
            "\n",
            "gunned on in in \n",
            "\n",
            "on \n",
            "\n",
            "in of \n",
            "\n",
            "in on \n",
            "\n",
            "on of . \n",
            "\n",
            ". of \n",
            "\n",
            "of until \n",
            "\n",
            "in of . \n",
            "\n",
            "on \n",
            "\n",
            "of of on in \n",
            "\n",
            "in \n",
            "\n",
            "on \n",
            "\n",
            "session in \n",
            "\n",
            "in of \n",
            "\n",
            "in of \n",
            "\n",
            "in in on \n",
            "\n",
            "on \n",
            "\n",
            "of in at \n",
            "\n",
            ", in \n",
            "\n",
            "in the of \n",
            "\n",
            "of on \n",
            "\n",
            "on \n",
            "\n",
            "of of of at - \n",
            "\n",
            "on of \n",
            "\n",
            "hit in \n",
            "\n",
            "on of on \n",
            "\n",
            "of of of of \n",
            "\n",
            "on for \n",
            "\n",
            "for on \n",
            "\n",
            "and , \n",
            "\n",
            "of \n",
            "\n",
            "in \n",
            "\n",
            "in \n",
            "\n",
            ", \n",
            "\n",
            "of in in \n",
            "\n",
            "on \n",
            "\n",
            "of \n",
            "\n",
            "dismissed for \n",
            "\n",
            "on \n",
            "\n",
            "of \n",
            "\n",
            "regional for of on on \n",
            "\n",
            "in \n",
            "\n",
            "information to on of on \n",
            "\n",
            "for of of in \n",
            "\n",
            "in in for \n",
            "\n",
            "of on \n",
            "\n",
            "of on \n",
            "\n",
            "on in for on \n",
            "\n",
            "in \n",
            "\n",
            "in economy \n",
            "\n",
            "in in of on \n",
            "\n",
            "on of in \n",
            "\n",
            "in for aimed \n",
            "\n",
            "of on \n",
            "\n",
            "who \n",
            "\n",
            "victory at \n",
            "\n",
            "in in of \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "on \n",
            "\n",
            "at in \n",
            "\n",
            "of for of \n",
            "\n",
            "of of in on \n",
            "\n",
            ". \n",
            "\n",
            "crowned of on in of in on \n",
            "\n",
            "of on \n",
            "\n",
            "of of \n",
            "\n",
            "for of of in in for \n",
            "\n",
            "of \n",
            "\n",
            "in of \n",
            "\n",
            "on in in \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "on \n",
            "\n",
            "in \n",
            "\n",
            "bull \n",
            "\n",
            "of at of \n",
            "\n",
            "for on of \n",
            "\n",
            "of in \n",
            "\n",
            "of there in \n",
            "\n",
            "in of on of \n",
            "\n",
            "of \n",
            "\n",
            "focus characters \n",
            "\n",
            "said \n",
            "\n",
            "of to of of on \n",
            "\n",
            "in deby \n",
            "\n",
            "in of \n",
            "\n",
            "at in truck of for \n",
            "\n",
            "in in \n",
            "\n",
            "of in for \n",
            "\n",
            "for of in \n",
            "\n",
            "for of \n",
            "\n",
            "of \n",
            "\n",
            "in \n",
            "\n",
            ", that for for \n",
            "\n",
            "on in for of of \n",
            "\n",
            "of \n",
            "\n",
            "for in \n",
            "\n",
            "of for \n",
            "\n",
            "on of \n",
            "\n",
            "of systems \n",
            "\n",
            "in in in in \n",
            "\n",
            ". of who at glasses \n",
            "\n",
            "for \n",
            "\n",
            "at of from \n",
            "\n",
            "of \n",
            "\n",
            "india of \n",
            "\n",
            "of on of in questions \n",
            "\n",
            "for on of \n",
            "\n",
            "of in of \n",
            "\n",
            "in \n",
            "\n",
            "on . in \n",
            "\n",
            "of at of on \n",
            "\n",
            "for of said \n",
            "\n",
            "by for in \n",
            "\n",
            "in of \n",
            "\n",
            "for of \n",
            "\n",
            "in \n",
            "\n",
            "for in \n",
            "\n",
            "of that \n",
            "\n",
            "vladimir related of \n",
            "\n",
            "spiritual at \n",
            "\n",
            "at for \n",
            "\n",
            "on of for the on \n",
            "\n",
            "in in of \n",
            "\n",
            "in where for \n",
            "\n",
            "of in <UNK> \n",
            "\n",
            "of \n",
            "\n",
            "in in technology of tons \n",
            "\n",
            "on \n",
            "\n",
            "of on \n",
            "\n",
            "on of for for \n",
            "\n",
            "on of <UNK> \n",
            "\n",
            "in on \n",
            "\n",
            "of , \n",
            "\n",
            "of on of \n",
            "\n",
            "in of , \n",
            "\n",
            "wednesday \n",
            "\n",
            "was of of on \n",
            "\n",
            "in of in of \n",
            "\n",
            "in for in \n",
            "\n",
            "' for of center of \n",
            "\n",
            "of \n",
            "\n",
            "for on \n",
            "\n",
            "of \n",
            "\n",
            "on for involving \n",
            "\n",
            "in \n",
            "\n",
            "of of on \n",
            "\n",
            "for on \n",
            "\n",
            "in in \n",
            "\n",
            "for of \n",
            "\n",
            "of in on \n",
            "\n",
            "to \n",
            "\n",
            "in in \n",
            "\n",
            "for of on \n",
            "\n",
            "in , on \n",
            "\n",
            "in , on \n",
            "\n",
            "of to in of \n",
            "\n",
            "on in \n",
            "\n",
            "and \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "of of \n",
            "\n",
            "in on \n",
            "\n",
            "of , of \n",
            "\n",
            "at from of for in \n",
            "\n",
            "in on \n",
            "\n",
            "for of \n",
            "\n",
            "on court \n",
            "\n",
            "of \n",
            "\n",
            "in \n",
            "\n",
            "for on \n",
            "\n",
            "at \n",
            "\n",
            "on \n",
            "\n",
            "allowed on \n",
            "\n",
            "at in by \n",
            "\n",
            "of of of \n",
            "\n",
            "are \n",
            "\n",
            "and of \n",
            "\n",
            "of \n",
            "\n",
            "and \n",
            "\n",
            "for \n",
            "\n",
            "on ' for \n",
            "\n",
            "in \n",
            "\n",
            "in can , in will \n",
            "\n",
            "asking for \n",
            "\n",
            "only people but of \n",
            "\n",
            "on \n",
            "\n",
            "on in of \n",
            "\n",
            "in as of \n",
            "\n",
            "of on \n",
            "\n",
            "of via \n",
            "\n",
            "of of in for for \n",
            "\n",
            "in forward \n",
            "\n",
            "on of \n",
            "\n",
            "in of \n",
            "\n",
            "on who of \n",
            "\n",
            "in on chain of in on \n",
            "\n",
            "in \n",
            "\n",
            "on on of \n",
            "\n",
            "in for in of \n",
            "\n",
            "for in of \n",
            "\n",
            "on of \n",
            "\n",
            "in \n",
            "\n",
            "on in \n",
            "\n",
            "in has for of \n",
            "\n",
            "of \n",
            "\n",
            "off of of \n",
            "\n",
            "your on on \n",
            "\n",
            "in since \n",
            "\n",
            "strike \n",
            "\n",
            "in on \n",
            "\n",
            "out , in \n",
            "\n",
            "for \n",
            "\n",
            "of \n",
            "\n",
            "diagnosed died \n",
            "\n",
            "of \n",
            "\n",
            "of \n",
            "\n",
            "at on \n",
            "\n",
            "for \n",
            "\n",
            "of of on on \n",
            "\n",
            "in of \n",
            "\n",
            "in we the \n",
            "\n",
            "on of in to title \n",
            "\n",
            "for \n",
            "\n",
            "in here , \n",
            "\n",
            "of for \n",
            "\n",
            "on for \n",
            "\n",
            "thursday in at \n",
            "\n",
            "in \n",
            "\n",
            "of of \n",
            "\n",
            "in of at of of \n",
            "\n",
            "on at \n",
            "\n",
            "at \n",
            "\n",
            "who of conversations \n",
            "\n",
            "at of \n",
            "\n",
            "at for \n",
            "\n",
            "on \n",
            "\n",
            "of of - on \n",
            "\n",
            "at , \n",
            "\n",
            "on alive \n",
            "\n",
            "at from more \n",
            "\n",
            "in in in of \n",
            "\n",
            "in on invasion of in on \n",
            "\n",
            "in in on \n",
            "\n",
            "into on \n",
            "\n",
            "for of \n",
            "\n",
            "said \n",
            "\n",
            "in for \n",
            "\n",
            "on in in in in \n",
            "\n",
            "per on on \n",
            "\n",
            "in of of of \n",
            "\n",
            "to in \n",
            "\n",
            "on for \n",
            "\n",
            "for -- she thursday in \n",
            "\n",
            "on \n",
            "\n",
            "of ' \n",
            "\n",
            "in in \n",
            "\n",
            "of on for \n",
            "\n",
            "in \n",
            "\n",
            "for for \n",
            "\n",
            "of of in \n",
            "\n",
            "of and \n",
            "\n",
            "in for of on \n",
            "\n",
            "in at on \n",
            "\n",
            "rrb on on \n",
            "\n",
            "for on of on \n",
            "\n",
            "for that of on \n",
            "\n",
            "of on , \n",
            "\n",
            "will \n",
            "\n",
            "at \n",
            "\n",
            "of three in on bangladesh \n",
            "\n",
            "of in on \n",
            "\n",
            "of in in \n",
            "\n",
            "for \n",
            "\n",
            "lrb on on \n",
            "\n",
            "monday for on \n",
            "\n",
            "in on in \n",
            "\n",
            "in in centred \n",
            "\n",
            "of of \n",
            "\n",
            "in of \n",
            "\n",
            "in in of \n",
            "\n",
            ". in \n",
            "\n",
            "in of \n",
            "\n",
            "for wednesday \n",
            "\n",
            "for of to \n",
            "\n",
            "in on of of of in \n",
            "\n",
            "on \n",
            "\n",
            "for of to \n",
            "\n",
            "propaganda of \n",
            "\n",
            "in in in on \n",
            "\n",
            "at on \n",
            "\n",
            "of on \n",
            "\n",
            "in at ' \n",
            "\n",
            "on collaboration of in \n",
            "\n",
            "in after of in of in \n",
            "\n",
            "years \n",
            "\n",
            "astronauts of . \n",
            "\n",
            "for for of \n",
            "\n",
            "for in on \n",
            "\n",
            "in home in in on who \n",
            "\n",
            "on of on \n",
            "\n",
            "to \n",
            "\n",
            "of of in \n",
            "\n",
            "that released for \n",
            "\n",
            "as of of tentative \n",
            "\n",
            "for on on on \n",
            "\n",
            "in on on \n",
            "\n",
            "in in \n",
            "\n",
            "in \n",
            "\n",
            "shot of \n",
            "\n",
            "in on on \n",
            "\n",
            "of of in of \n",
            "\n",
            "of in of \n",
            "\n",
            "of of of on \n",
            "\n",
            "of \n",
            "\n",
            "for of \n",
            "\n",
            "of on \n",
            "\n",
            "of in \n",
            "\n",
            "in with radovan on \n",
            "\n",
            "of of area in on \n",
            "\n",
            "on on of of \n",
            "\n",
            "of \n",
            "\n",
            "in on of on on \n",
            "\n",
            "in in \n",
            "\n",
            "of of of \n",
            "\n",
            "in \n",
            "\n",
            "for on in \n",
            "\n",
            ", \n",
            "\n",
            "of \n",
            "\n",
            "in in \n",
            "\n",
            "for on of in \n",
            "\n",
            "at of at of on at \n",
            "\n",
            "for for in in \n",
            "\n",
            "wounded in for of \n",
            "\n",
            "on \n",
            "\n",
            "of into on \n",
            "\n",
            "on on \n",
            "\n",
            "of on \n",
            "\n",
            "on \n",
            "\n",
            ", \n",
            "\n",
            "on in of \n",
            "\n",
            "here \n",
            "\n",
            ". of \n",
            "\n",
            "said of in by \n",
            "\n",
            "on \n",
            "\n",
            "of on \n",
            "\n",
            "on \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "on of \n",
            "\n",
            "at on \n",
            "\n",
            "on \n",
            "\n",
            ". at \n",
            "\n",
            "of on in in for \n",
            "\n",
            "on on for for \n",
            "\n",
            "about of \n",
            "\n",
            "at of of \n",
            "\n",
            "of of \n",
            "\n",
            "on in in \n",
            "\n",
            "at at \n",
            "\n",
            "in on in \n",
            "\n",
            "at in \n",
            "\n",
            "unless at alive \n",
            "\n",
            "at for for of \n",
            "\n",
            "wednesday in in of \n",
            "\n",
            "at of at for \n",
            "\n",
            ", of \n",
            "\n",
            "in \n",
            "\n",
            "on \n",
            "\n",
            "of \n",
            "\n",
            "on \n",
            "\n",
            "in for \n",
            "\n",
            "said of in on \n",
            "\n",
            "for \n",
            "\n",
            "of \n",
            "\n",
            "on \n",
            "\n",
            "thursday in in of \n",
            "\n",
            "their . \n",
            "\n",
            "on \n",
            "\n",
            "at of \n",
            "\n",
            "unless at as ' \n",
            "\n",
            "of of on \n",
            "\n",
            "friday at on \n",
            "\n",
            "as \n",
            "\n",
            "in in \n",
            "\n",
            "in for in \n",
            "\n",
            "percent to on \n",
            "\n",
            "was of \n",
            "\n",
            "of by . \n",
            "\n",
            "in on \n",
            "\n",
            "for in . \n",
            "\n",
            "on in \n",
            "\n",
            "on of of \n",
            "\n",
            "on for centred in \n",
            "\n",
            "of of \n",
            "\n",
            "of on on \n",
            "\n",
            "in to \n",
            "\n",
            "in of emerging \n",
            "\n",
            "for on \n",
            "\n",
            "in of \n",
            "\n",
            "on \n",
            "\n",
            "of of \n",
            "\n",
            "to of on \n",
            "\n",
            "in of in of of \n",
            "\n",
            "of \n",
            "\n",
            "on beijing . \n",
            "\n",
            "for ' on \n",
            "\n",
            ", \n",
            "\n",
            "on \n",
            "\n",
            "of \n",
            "\n",
            "of in \n",
            "\n",
            "in \n",
            "\n",
            "in \n",
            "\n",
            "of of \n",
            "\n",
            "from by of \n",
            "\n",
            "of \n",
            "\n",
            "<UNK> in \n",
            "\n",
            "on \n",
            "\n",
            "on here \n",
            "\n",
            "in \n",
            "\n",
            "in in in \n",
            "\n",
            "in of \n",
            "\n",
            "in for \n",
            "\n",
            "of \n",
            "\n",
            "in of of of \n",
            "\n",
            "in of in said of \n",
            "\n",
            "at of \n",
            "\n",
            "in of \n",
            "\n",
            "in for \n",
            "\n",
            "on of for for on \n",
            "\n",
            "for of \n",
            "\n",
            "within \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            ", of \n",
            "\n",
            "on on \n",
            "\n",
            "for \n",
            "\n",
            "on in for at \n",
            "\n",
            "of of \n",
            "\n",
            "on on in \n",
            "\n",
            "in \n",
            "\n",
            "in for \n",
            "\n",
            ", are \n",
            "\n",
            "pavel in in of ' \n",
            "\n",
            "on \n",
            "\n",
            "in in the \n",
            "\n",
            "on for of \n",
            "\n",
            "if of in on \n",
            "\n",
            "plane \n",
            "\n",
            "in of for \n",
            "\n",
            "in of \n",
            "\n",
            "of in \n",
            "\n",
            "for \n",
            "\n",
            "in , \n",
            "\n",
            "for on tool on \n",
            "\n",
            "of in manned in \n",
            "\n",
            "on friday \n",
            "\n",
            "on \n",
            "\n",
            "in - for of \n",
            "\n",
            "on on \n",
            "\n",
            "of of \n",
            "\n",
            "on of on \n",
            "\n",
            "of on \n",
            "\n",
            "in \n",
            "\n",
            "on in in of \n",
            "\n",
            "on \n",
            "\n",
            "for of of \n",
            "\n",
            "on of of of \n",
            "\n",
            ", ' \n",
            "\n",
            "of of \n",
            "\n",
            "of in of in \n",
            "\n",
            "of of restriction of \n",
            "\n",
            "of of \n",
            "\n",
            "in \n",
            "\n",
            "on \n",
            "\n",
            "in here on \n",
            "\n",
            "in of of \n",
            "\n",
            "traveled of of \n",
            "\n",
            "review at \n",
            "\n",
            "of of , \n",
            "\n",
            "in of \n",
            "\n",
            "<UNK> , of \n",
            "\n",
            "of for on , of \n",
            "\n",
            "in sinlaku . to \n",
            "\n",
            "of . \n",
            "\n",
            "for of on \n",
            "\n"
          ]
        }
      ],
      "source": [
        "train = PlaintextCorpusReader(\"./\",\"dev.in\")\n",
        "sents = train.sents()\n",
        "f = open(\"./dd.out\",\"w+\")\n",
        "\n",
        "\n",
        "for sent in sents:\n",
        "    for i in range(len(sent)):\n",
        "        if sent[i] == \"PREP\":\n",
        "            # print(sent[i-2],sent[i+2])\n",
        "            ans = tri_StupidBackoff_model.generate(text_seed=[sent[i-4],sent[i-3],sent[i-2]])\n",
        "            # ans = tri_StupidBackoff_model.generate(text_seed=[sent[i-3],sent[i-2]])\n",
        "            # ans = bi_StupidBackoff_model.generate(text_seed=[sent[i-3],sent[i-2]])\n",
        "            # ans = bi_StupidBackoff_model.generate(text_seed=sent[i-2])\n",
        "            num = 0\n",
        "            while ans not in [\"at\", \"in\", \"of\", \"for\", \"on\"] and num <100:\n",
        "                ans = tri_StupidBackoff_model.generate(text_seed=[sent[i-4],sent[i-3],sent[i-2]])\n",
        "                # ans = tri_StupidBackoff_model.generate(text_seed=[sent[i-3],sent[i-2]])\n",
        "                # ans = bi_StupidBackoff_model.generate(text_seed=[sent[i-3],sent[i-2]])\n",
        "                # ans = bi_StupidBackoff_model.generate(text_seed=sent[i-2])\n",
        "                num += 1\n",
        "            print(ans,end=\" \")\n",
        "            f.write(str(ans)+\" \")\n",
        "    print(\"\\n\")\n",
        "    f.write(\"\\n\")\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtvE4K3G827B",
        "outputId": "884dbdf5-064c-4d7e-90cb-6100abcea96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groundtruth: ['on', 'of', 'in', 'in', 'of', 'of', 'in', 'of', 'for', 'of', 'in', 'for', 'on', 'at', 'in', 'on', 'of', 'in', 'in', 'on', 'of', 'on', 'on', 'in', 'on', 'on', 'of', 'on', 'of', 'in', 'in', 'for', 'in', 'of', 'on', 'in', 'of', 'on', 'of', 'in', 'of', 'on', 'for', 'of', 'of', 'at', 'on', 'at', 'on', 'in', 'of', 'of', 'of', 'in', 'on', 'on', 'on', 'for', 'of', 'in', 'in', 'at', 'of', 'for', 'on', 'for', 'of', 'on', 'at', 'of', 'on', 'in', 'of', 'of', 'in', 'on', 'at', 'on', 'on', 'in', 'in', 'of', 'on', 'on', 'in', 'of', 'in', 'of', 'on', 'of', 'on', 'of', 'in', 'in', 'in', 'at', 'of', 'at', 'of', 'on', 'of', 'in', 'of', 'on', 'for', 'for', 'in', 'of', 'on', 'at', 'on', 'on', 'on', 'of', 'for', 'in', 'of', 'in', 'on', 'of', 'on', 'of', 'of', 'for', 'in', 'of', 'on', 'of', 'in', 'for', 'in', 'in', 'at', 'of', 'in', 'of', 'in', 'in', 'on', 'in', 'in', 'in', 'at', 'on', 'at', 'of', 'in', 'of', 'in', 'on', 'in', 'in', 'for', 'for', 'of', 'of', 'at', 'in', 'at', 'of', 'on', 'in', 'at', 'in', 'at', 'on', 'for', 'at', 'in', 'on', 'on', 'in', 'at', 'in', 'on', 'on', 'of', 'on', 'of', 'on', 'in', 'on', 'for', 'in', 'for', 'for', 'for', 'in', 'of', 'for', 'in', 'in', 'of', 'of', 'in', 'of', 'in', 'on', 'in', 'for', 'in', 'of', 'for', 'at', 'on', 'of', 'at', 'on', 'for', 'in', 'for', 'on', 'of', 'in', 'of', 'of', 'of', 'of', 'in', 'on', 'of', 'of', 'at', 'for', 'of', 'for', 'of', 'at', 'in', 'of', 'of', 'on', 'of', 'at', 'in', 'for', 'of', 'of', 'at', 'of', 'in', 'of', 'in', 'of', 'for', 'of', 'of', 'of', 'of', 'on', 'of', 'in', 'in', 'at', 'of', 'in', 'of', 'in', 'of', 'of', 'at', 'on', 'for', 'in', 'of', 'on', 'of', 'in', 'in', 'on', 'of', 'on', 'in', 'of', 'of', 'on', 'in', 'of', 'in', 'for', 'of', 'at', 'for', 'of', 'on', 'of', 'of', 'on', 'on', 'of', 'of', 'at', 'of', 'on', 'in', 'of', 'in', 'for', 'of', 'in', 'at', 'of', 'at', 'on', 'of', 'of', 'in', 'of', 'on', 'of', 'of', 'of', 'in', 'on', 'for', 'of', 'on', 'in', 'on', 'in', 'of', 'in', 'of', 'on', 'of', 'in', 'of', 'of', 'in', 'of', 'of', 'of', 'at', 'in', 'of', 'of', 'of', 'in', 'on', 'at', 'on', 'of', 'of', 'in', 'of', 'in', 'on', 'of', 'in', 'in', 'of', 'in', 'for', 'on', 'of', 'in', 'of', 'of', 'in', 'of', 'in', 'on', 'on', 'of', 'in', 'on', 'for', 'in', 'of', 'on', 'of', 'of', 'for', 'at', 'on', 'in', 'on', 'at', 'in', 'at', 'on', 'of', 'in', 'for', 'for', 'on', 'of', 'at', 'of', 'for', 'in', 'of', 'for', 'in', 'in', 'on', 'on', 'in', 'on', 'for', 'in', 'of', 'for', 'in', 'of', 'in', 'on', 'of', 'in', 'of', 'on', 'in', 'of', 'for', 'in', 'at', 'in', 'on', 'on', 'on', 'of', 'on', 'for', 'at', 'on', 'on', 'in', 'on', 'of', 'of', 'at', 'of', 'in', 'in', 'at', 'of', 'on', 'on', 'in', 'in', 'of', 'on', 'on', 'on', 'of', 'in', 'in', 'of', 'of', 'for', 'in', 'for', 'for', 'of', 'at', 'in', 'in', 'in', 'of', 'in', 'on', 'of', 'in', 'of', 'of', 'on', 'on', 'on', 'for', 'of', 'of', 'in', 'in', 'on', 'in', 'on', 'of', 'of', 'for', 'in', 'in', 'of', 'on', 'in', 'of', 'for', 'of', 'of', 'of', 'on', 'on', 'in', 'in', 'of', 'of', 'for', 'in', 'on', 'in', 'for', 'of', 'in', 'of', 'on', 'for', 'at', 'on', 'on', 'for', 'of', 'in', 'of', 'on', 'in', 'for', 'in', 'of', 'of', 'of', 'on', 'in', 'of', 'for', 'on', 'in', 'in', 'for', 'on', 'on', 'of', 'for', 'on', 'of', 'in', 'of', 'on', 'on', 'on', 'in', 'of', 'on', 'in', 'for', 'on', 'of', 'in', 'of', 'on', 'in', 'on', 'in', 'of', 'in', 'of', 'in', 'for', 'in', 'on', 'of', 'on', 'at', 'in', 'on', 'at', 'in', 'of', 'of', 'of', 'in', 'in', 'in', 'in', 'in', 'in', 'of', 'in', 'of', 'on', 'for', 'in', 'in', 'in', 'of', 'in', 'in', 'in', 'of', 'in', 'of', 'of', 'of', 'of', 'in', 'in', 'of', 'for', 'of', 'in', 'of', 'in', 'on', 'in', 'on', 'in', 'for', 'of', 'at', 'of', 'on', 'of', 'on', 'of', 'of', 'in', 'of', 'on', 'in', 'of', 'in', 'at', 'of', 'in', 'in', 'in', 'on', 'of', 'on', 'of', 'on', 'in', 'on', 'on', 'in', 'of', 'in', 'in', 'in', 'for', 'for', 'for', 'in', 'on', 'of', 'at', 'on', 'in', 'in', 'in', 'for', 'on', 'on', 'on', 'for', 'of', 'on', 'for', 'in', 'in', 'for', 'of', 'in', 'at', 'of', 'for', 'on', 'of', 'for', 'in', 'at', 'of', 'of', 'on', 'of', 'for', 'in', 'in', 'in', 'at', 'of', 'of', 'on', 'for', 'of', 'of', 'in', 'for', 'of', 'in', 'in', 'at', 'for', 'for', 'on', 'of', 'at', 'in', 'on', 'in', 'of', 'of', 'on', 'of', 'of', 'for', 'in', 'of', 'in', 'of', 'of', 'of', 'in', 'for', 'on', 'at', 'in', 'for', 'of', 'of', 'in', 'on', 'at', 'for', 'of', 'of', 'on', 'for', 'in', 'for', 'of', 'in', 'in', 'of', 'in', 'for', 'in', 'of', 'of', 'in', 'of', 'of', 'of', 'in', 'in', 'of', 'for', 'for', 'on', 'of', 'for', 'of', 'at', 'for', 'at', 'at', 'of', 'on', 'for', 'on', 'in', 'in', 'in', 'of', 'in', 'on', 'in', 'in', 'on', 'in', 'on', 'of', 'of', 'on', 'in', 'of', 'in', 'in', 'in', 'at', 'for', 'of', 'on', 'at', 'in', 'on', 'in', 'of', 'of', 'of', 'in', 'in', 'of', 'in', 'in', 'on', 'in', 'in', 'in', 'on', 'on', 'of', 'in', 'on', 'in', 'of', 'in', 'on', 'in', 'in', 'in', 'in', 'of', 'on', 'of', 'in', 'on', 'in', 'on', 'of', 'in', 'in', 'for', 'of', 'on', 'at', 'in', 'on', 'on', 'at', 'of', 'for', 'of', 'of', 'of', 'in', 'in', 'of', 'of', 'of', 'in', 'on', 'of', 'of', 'of', 'in', 'of', 'on', 'in', 'on', 'in', 'on', 'in', 'of', 'of', 'of', 'in', 'for', 'of', 'of', 'on', 'of', 'of', 'on', 'in', 'in', 'in', 'for', 'in', 'for', 'at', 'of', 'on', 'of', 'on', 'in', 'in', 'in', 'on', 'on', 'on', 'of', 'in', 'in', 'in', 'in', 'on', 'on', 'on', 'in', 'on', 'on', 'in', 'of', 'for', 'of', 'in', 'of', 'on', 'in', 'in', 'in', 'on', 'of', 'of', 'in', 'in', 'in', 'in', 'at', 'of', 'in', 'for', 'in', 'on', 'for', 'at', 'on', 'for', 'on', 'at', 'at', 'of', 'in', 'of', 'on', 'of', 'in', 'in', 'in', 'of', 'of', 'in', 'on', 'in', 'in', 'for', 'of', 'on', 'for', 'on', 'of', 'in', 'at', 'in', 'of', 'on', 'on', 'of', 'for', 'at', 'at', 'of', 'in', 'at', 'on', 'on', 'of', 'at', 'for', 'at', 'in', 'for', 'on', 'of', 'at', 'in', 'of', 'in', 'of', 'on', 'of', 'of', 'in', 'of', 'of', 'in', 'of', 'of', 'in', 'for', 'for', 'in', 'of', 'on', 'on', 'of', 'of', 'at', 'of', 'in', 'of', 'on', 'on', 'of', 'in', 'for', 'for', 'of', 'in', 'of', 'of', 'for', 'for', 'in', 'in', 'in', 'on', 'of', 'at', 'of', 'of', 'in', 'for', 'of', 'of', 'on', 'of', 'for', 'on', 'in', 'of', 'on', 'of', 'in', 'in', 'in', 'on', 'in', 'of', 'of', 'in', 'in', 'at', 'of', 'in', 'of', 'in', 'of', 'of', 'in', 'in', 'of', 'for', 'of', 'in', 'of', 'in', 'of', 'in', 'of', 'of', 'on', 'in', 'of', 'on', 'in', 'in', 'for', 'in', 'for', 'on', 'of', 'in', 'of', 'in', 'of', 'of', 'in', 'in', 'for', 'in', 'at', 'of', 'in', 'in', 'of', 'on', 'of', 'at', 'in', 'for', 'of', 'in', 'of', 'on', 'of', 'of', 'in', 'of', 'of', 'in', 'in', 'in', 'of', 'of', 'in', 'for', 'of', 'for', 'in', 'of', 'in', 'on', 'in', 'of', 'of', 'of', 'of', 'on', 'in', 'on', 'in', 'at', 'of', 'on', 'of', 'of', 'on', 'of', 'at', 'of', 'on', 'of', 'of', 'for', 'in', 'of', 'in', 'in', 'on', 'in', 'for', 'in', 'of', 'on', 'of', 'in', 'on', 'on', 'of', 'in', 'of', 'at', 'on', 'of', 'at', 'of', 'in', 'of', 'for', 'on', 'in', 'of', 'of', 'of', 'in', 'in', 'for', 'in', 'on', 'for', 'in', 'of', 'on', 'of', 'in', 'on', 'of', 'in', 'in', 'on', 'on', 'on', 'for', 'on', 'of', 'of', 'of', 'in', 'for', 'in', 'of', 'in', 'of', 'in', 'in', 'on', 'of', 'for', 'of', 'of', 'in', 'in', 'in', 'on', 'on', 'of', 'in', 'of', 'in', 'of', 'on', 'of', 'of', 'in', 'on', 'in', 'of', 'at', 'on', 'at', 'on', 'on', 'for', 'of', 'in', 'in', 'on', 'at', 'of', 'in', 'in', 'of', 'in', 'for', 'in', 'of', 'on', 'in', 'of', 'on', 'in', 'on', 'for', 'in', 'of', 'for', 'of', 'in', 'on', 'in', 'on', 'at', 'of', 'in', 'in', 'of', 'in', 'of', 'in', 'of', 'for', 'on', 'of', 'of', 'of', 'on', 'of', 'for', 'on', 'at', 'of', 'of', 'on', 'of', 'of', 'of', 'of', 'in', 'of', 'on', 'of', 'of', 'of', 'at', 'on', 'of', 'in', 'of', 'in', 'of', 'of', 'in', 'on', 'in', 'of', 'of', 'in', 'in', 'of', 'in', 'in', 'of', 'for', 'of', 'of', 'for', 'in', 'at', 'for', 'of', 'at', 'in', 'at', 'in', 'in', 'in', 'of', 'on', 'of', 'of', 'of', 'of', 'in', 'in', 'in', 'of', 'in', 'in', 'in', 'for', 'in', 'in', 'of', 'in', 'in', 'of', 'in', 'of', 'in', 'in', 'at', 'of', 'of', 'on', 'for', 'in', 'in', 'for', 'of', 'of', 'on', 'at', 'in', 'of', 'of', 'in', 'for', 'in', 'in', 'in', 'on', 'in', 'in', 'of', 'in', 'of', 'on', 'on', 'of', 'for', 'for', 'in', 'on', 'on', 'of', 'on', 'of', 'on', 'of', 'in', 'of', 'in', 'of', 'of', 'of', 'in', 'of', 'on', 'of', 'for', 'of', 'in', 'for', 'of', 'for', 'in', 'on', 'in', 'in', 'on', 'for', 'in', 'of', 'in', 'on', 'in', 'at', 'for', 'of', 'on', 'for', 'of', 'of', 'in', 'for', 'of', 'for', 'in', 'on', 'for', 'of', 'for', 'for', 'on', 'on', 'of', 'on', 'of', 'on', 'in', 'for', 'for', 'of', 'in', 'in', 'in', 'at', 'on', 'on', 'of', 'of', 'on', 'at', 'of', 'in', 'in', 'of', 'of', 'of', 'in', 'of', 'of', 'in', 'in', 'for', 'on', 'in', 'in', 'in', 'of', 'on', 'of', 'at', 'of', 'in', 'on', 'in', 'of', 'of', 'in', 'on', 'of', 'in', 'in', 'of', 'on', 'of', 'in', 'in', 'of', 'in', 'of', 'in', 'of', 'of', 'on', 'of', 'in', 'in', 'in', 'in', 'of', 'for', 'of', 'of', 'of', 'in', 'in', 'on', 'on', 'in', 'of', 'of', 'in', 'in', 'on', 'in', 'in', 'in', 'of', 'in', 'of', 'in', 'on', 'on', 'of', 'at', 'in', 'for', 'of', 'in', 'of', 'in', 'in', 'in', 'of', 'for', 'of', 'of', 'at', 'of', 'for', 'of', 'of', 'for', 'of', 'on', 'of', 'in', 'for', 'of', 'in', 'of', 'on', 'for', 'for', 'in', 'in', 'on', 'in', 'in', 'in', 'of', 'on', 'in', 'for', 'in', 'of', 'in', 'of', 'of', 'of', 'on', 'for', 'in', 'for', 'in', 'at', 'in', 'of', 'in', 'of', 'in', 'at', 'at', 'on', 'of', 'of', 'in', 'for', 'in', 'at', 'on', 'on', 'on', 'of', 'at', 'for', 'of', 'on', 'for', 'on', 'in', 'of', 'in', 'on', 'in', 'of', 'in', 'of', 'of', 'of', 'for', 'of', 'in', 'in', 'in', 'on', 'for', 'in', 'at', 'on', 'in', 'in', 'of', 'in', 'of', 'of', 'for', 'on', 'in', 'on', 'in', 'in', 'of', 'of', 'on', 'in', 'of', 'of', 'of', 'in', 'in', 'on', 'in', 'on', 'of', 'for', 'for', 'in', 'in', 'of', 'of', 'on', 'on', 'of', 'at', 'of', 'on', 'on', 'on', 'in', 'of', 'in', 'of', 'in', 'in', 'in', 'on', 'of', 'of', 'in', 'of', 'for', 'of', 'of', 'on', 'of', 'in', 'on', 'of', 'at', 'on', 'in', 'for', 'in', 'at', 'of', 'on', 'of', 'in', 'on', 'on', 'of', 'of', 'of', 'of', 'on', 'in', 'in', 'of', 'in', 'on', 'on', 'of', 'at', 'in', 'for', 'on', 'in', 'on', 'in', 'for', 'of', 'of', 'of', 'in', 'in', 'on', 'of', 'in', 'at', 'of', 'of', 'on', 'of', 'of', 'of', 'on', 'on', 'of', 'of', 'of', 'in', 'at', 'of', 'of', 'on', 'of', 'in', 'of', 'on', 'of', 'in', 'of', 'on', 'on', 'at', 'in', 'on', 'for', 'in', 'of', 'on', 'on', 'in', 'of', 'at', 'in', 'on', 'of', 'in', 'of', 'on', 'of', 'on', 'of', 'of', 'at', 'in', 'for', 'in', 'at', 'in', 'of', 'in', 'for', 'of', 'of', 'in', 'in', 'on', 'in', 'of', 'in', 'of', 'in', 'for', 'in', 'in', 'in', 'in', 'in', 'on', 'in', 'for', 'of', 'on', 'on', 'on', 'in', 'in', 'for', 'on', 'of', 'on', 'in', 'on', 'in', 'of', 'in', 'in', 'in', 'at', 'on', 'at', 'on', 'for', 'on', 'of', 'of', 'of', 'in', 'on', 'on', 'for', 'at', 'in', 'on', 'of', 'at', 'on', 'of', 'on', 'of', 'of', 'at', 'of', 'of', 'in', 'on', 'of', 'of', 'in', 'of', 'on', 'on', 'in', 'of', 'of', 'of', 'in', 'of', 'of', 'in', 'of', 'for', 'in', 'on', 'in', 'of', 'of', 'in', 'for', 'in', 'of', 'on', 'of', 'on', 'on', 'in', 'on', 'of', 'for', 'on', 'of', 'on', 'in', 'on', 'on', 'of', 'for', 'in', 'on', 'of', 'at', 'in', 'of', 'of', 'at', 'in', 'in', 'of', 'in', 'for', 'at', 'of', 'in', 'on', 'for', 'on', 'for', 'at', 'on', 'for', 'of', 'of', 'of', 'for', 'in', 'of', 'of', 'in', 'of', 'of', 'of', 'in', 'in', 'on', 'in', 'of', 'of', 'at', 'in', 'of', 'for', 'in', 'of', 'in', 'of', 'of', 'of', 'of', 'on', 'of', 'in', 'in', 'on', 'at', 'of', 'for', 'for', 'of', 'for', 'for', 'for', 'in', 'of', 'in', 'of', 'on', 'of', 'of', 'on', 'in', 'on', 'of', 'in', 'of', 'in', 'of', 'in', 'in', 'of', 'of', 'of', 'in', 'in', 'of', 'in', 'in', 'in', 'for', 'on', 'of', 'in', 'for', 'on', 'at', 'in', 'of', 'in', 'in', 'on', 'of', 'in', 'on', 'in', 'for', 'for', 'on', 'of', 'at', 'in', 'of', 'in', 'on', 'of', 'on', 'for', 'on', 'of', 'on', 'in', 'in', 'in', 'on', 'for', 'at', 'for', 'for', 'of', 'in', 'on', 'of', 'of', 'of', 'on', 'in', 'of', 'at', 'of', 'at', 'in', 'in', 'in', 'on', 'for', 'on', 'of', 'of', 'in', 'in', 'for', 'on', 'of', 'in', 'at', 'of', 'of', 'in', 'of', 'in', 'of', 'in', 'for', 'of', 'of', 'on', 'in', 'of', 'in', 'in', 'for', 'of', 'in', 'in', 'in', 'on', 'in', 'in', 'on', 'of', 'on', 'in', 'of', 'on', 'of', 'in', 'in', 'of', 'in', 'of', 'in', 'in', 'of', 'in', 'of', 'in', 'of', 'of', 'of', 'at', 'in', 'on', 'for', 'of', 'on', 'in', 'of', 'in', 'on', 'in', 'on', 'in', 'in', 'on', 'of', 'in', 'in', 'of', 'of', 'of', 'in', 'at', 'for', 'of', 'for', 'for', 'on', 'in', 'for', 'in', 'on', 'in', 'on', 'of', 'for', 'on', 'in', 'for', 'of', 'on', 'of', 'on', 'of', 'in', 'of', 'in', 'in', 'for', 'of', 'on', 'of', 'in', 'of', 'on', 'in', 'on', 'in', 'in', 'of', 'on', 'at', 'in', 'of', 'on', 'in', 'of', 'at', 'in', 'on', 'of', 'at', 'on', 'for', 'in', 'on', 'of', 'of', 'in', 'on', 'for', 'for', 'for', 'of', 'of', 'of', 'on', 'in', 'of', 'on', 'in', 'in', 'of', 'on', 'in', 'of', 'in', 'in', 'on', 'on', 'in', 'on', 'for', 'of', 'in', 'in', 'for', 'at', 'for', 'of', 'in', 'at', 'on', 'of', 'for', 'on', 'of', 'of', 'in', 'at', 'in', 'in', 'at', 'on', 'of', 'in', 'of', 'of', 'in', 'of', 'at', 'at', 'in', 'on', 'of', 'of', 'for', 'on', 'for', 'for', 'for', 'at', 'in', 'in', 'of', 'of', 'of', 'in', 'in', 'of', 'of', 'at', 'in', 'in', 'for', 'in', 'on', 'of', 'in', 'for', 'at', 'of', 'of', 'in', 'on', 'in', 'for', 'of', 'of', 'at', 'of', 'for', 'of', 'in', 'for', 'of', 'at', 'in', 'on', 'of', 'at', 'on', 'in', 'in', 'on', 'on', 'at', 'in', 'of', 'in', 'on', 'for', 'for', 'of', 'on', 'for', 'in', 'of', 'in', 'on', 'in', 'of', 'of', 'on', 'of', 'at', 'for', 'for', 'for', 'on', 'of', 'for', 'on', 'at', 'of', 'at', 'in', 'in', 'of', 'on', 'for', 'in', 'at', 'in', 'of', 'of', 'in', 'of', 'in', 'for', 'of', 'of', 'on', 'in', 'at', 'on', 'in', 'at', 'on', 'for', 'in', 'of', 'on', 'in', 'of', 'of', 'in', 'of', 'on', 'of', 'in', 'in', 'of', 'at', 'on', 'in', 'on', 'of', 'on', 'at', 'of', 'for', 'in', 'on', 'in', 'at', 'on', 'on', 'on', 'for', 'in', 'at', 'in', 'for', 'of', 'in', 'of', 'of', 'in', 'of', 'on', 'in', 'of', 'of', 'in', 'on', 'of', 'for', 'in', 'for', 'of', 'for', 'in', 'on', 'of', 'for', 'of', 'in', 'on', 'for', 'of', 'for', 'in', 'of', 'of', 'on', 'of', 'in', 'of', 'at', 'of', 'in', 'on', 'of', 'on', 'for', 'for', 'in', 'at', 'in', 'on', 'of', 'for', 'of', 'in', 'at', 'of', 'at', 'in', 'for', 'on', 'of', 'for', 'of', 'in', 'in', 'of', 'in', 'in', 'on', 'in', 'on', 'of', 'of', 'on', 'of', 'on', 'in', 'in', 'in', 'for', 'in', 'for', 'in', 'at', 'for', 'in', 'in', 'in', 'of', 'on', 'in', 'in', 'in', 'in', 'in', 'of', 'in', 'in', 'for', 'of', 'of', 'in', 'of', 'of', 'in', 'of', 'in', 'of', 'of', 'on', 'in', 'in', 'of', 'in', 'on', 'of', 'in', 'in', 'for', 'in', 'on', 'of', 'of', 'on', 'in', 'in', 'for', 'in', 'in', 'of', 'in', 'at', 'on', 'of', 'at', 'on', 'in', 'in', 'of', 'in', 'of', 'at', 'for', 'at', 'in', 'in', 'of', 'of', 'in', 'for', 'on', 'of', 'in', 'at', 'in', 'in', 'of', 'in', 'at', 'for', 'for', 'of', 'in', 'at', 'in', 'in', 'of', 'on', 'of', 'in', 'on', 'on', 'in', 'of', 'on', 'on', 'of', 'in', 'of', 'on', 'at', 'in', 'of', 'in', 'of', 'in', 'on', 'on', 'of', 'on', 'on', 'on', 'in', 'on', 'of', 'on', 'at', 'in', 'of', 'of', 'of', 'in', 'in', 'for', 'for', 'on', 'on', 'for', 'in', 'at', 'on', 'on', 'of', 'of', 'on', 'of', 'in', 'in', 'in', 'in', 'on', 'for', 'on', 'in', 'on', 'in', 'in', 'on', 'at', 'for', 'of', 'for', 'of', 'on', 'at', 'in', 'of', 'for', 'of', 'in', 'of', 'in', 'in', 'in', 'at', 'of', 'on', 'of', 'in', 'on', 'of', 'on', 'for', 'of', 'in', 'in', 'on', 'at', 'in', 'of', 'on', 'in', 'of', 'in', 'for', 'in', 'on', 'of', 'at', 'at', 'of', 'in', 'at', 'in', 'on', 'for', 'on', 'on', 'on', 'for', 'in', 'in', 'for', 'on', 'at', 'in', 'in', 'for', 'on', 'in', 'on', 'for', 'in', 'in', 'in', 'in', 'on', 'of', 'in', 'on', 'for', 'on', 'in', 'of', 'of', 'of', 'on', 'in', 'for', 'in', 'in', 'of', 'in', 'for', 'on', 'at', 'of', 'for', 'of', 'of', 'in', 'of', 'on', 'on', 'of', 'at', 'of', 'in', 'in', 'on', 'of', 'in', 'in', 'in', 'on', 'in', 'at', 'of', 'of', 'in', 'in', 'of', 'at', 'of', 'on', 'on', 'of', 'on', 'of', 'for', 'in', 'on', 'on', 'in', 'in', 'in', 'of', 'in', 'of', 'for', 'of', 'of', 'in', 'of', 'of', 'of', 'in', 'of', 'in', 'in', 'of', 'of', 'for', 'at', 'of', 'on', 'for', 'of', 'of', 'at', 'for', 'on', 'for', 'on', 'in', 'in', 'of', 'in', 'of', 'on', 'on', 'for', 'in', 'of', 'on', 'at', 'on', 'of', 'of', 'on', 'in', 'in', 'on', 'for', 'in', 'of', 'in', 'in', 'for', 'of', 'on', 'on', 'in', 'in', 'in', 'for', 'in', 'for', 'on', 'of', 'of', 'on', 'of', 'on', 'of', 'in', 'in', 'of', 'on', 'in', 'on', 'of', 'in', 'for', 'on', 'of', 'in', 'of', 'in', 'in', 'in', 'on', 'in', 'on', 'for', 'in', 'at', 'of', 'on', 'on', 'of', 'for', 'on', 'of', 'of', 'of', 'for', 'on', 'on', 'in', 'for', 'in', 'on', 'of', 'of', 'of', 'on', 'of', 'in', 'of', 'in', 'in', 'of', 'of', 'at', 'in', 'of', 'at', 'of', 'of', 'of', 'of', 'of', 'of', 'in', 'on', 'in', 'on', 'on', 'in', 'of', 'for', 'on', 'on', 'on', 'of', 'at', 'of', 'of', 'of', 'of', 'of', 'on', 'of', 'of', 'on', 'for', 'on', 'in', 'of', 'for', 'of', 'of', 'of', 'in', 'of', 'on', 'of', 'in']\n",
            "Prediction: ['at', 'of', 'this', 'at', 'of', 'of', 'destroyed', 'of', 'on', 'of', 'in', 'for', 'on', 'is', 'for', 'on', 'of', 'in', 'in', 'of', 'of', 'on', 'of', 'to', 'of', 'in', 'of', 'of', 'of', ',', 'in', 'in', 'for', 'of', 'in', 'in', 'of', 'available', 'of', '.', 'of', 'in', 'at', 'of', 'of', 'to', 'at', 'at', '<UNK>', 'in', 'in', 'of', 'in', 'of', 'of', '.', 'into', 'for', 'of', 'on', 'in', 'for', 'of', 'for', 'at', 'in', 'of', 'level', 'at', 'in', 'in', 'at', 'of', 'adulyadej', 'in', '.', 'of', 'on', 'for', 'for', 'being', 'of', 'on', ',', 'so', 'of', 'in', 'of', 'of', 'of', 'in', 'of', 'outside', 'in', 'in', 'in', 'of', 'in', 'of', 'on', 'symptoms', 'in', 'of', 'in', 'on', 'older', 'in', 'of', 'in', '.', 'friendly', 'has', ',', 'of', 'to', 'of', 'on', 'in', 'financial', 'of', 'for', 'of', 'of', 'of', 'being', 'of', 'on', 'of', 'in', 'with', 'in', 'it', 'over', 'of', 'of', 'of', 'on', 'in', 'on', 'in', 'in', 'in', 'at', 'for', 'entered', 'of', 'in', 'of', 'in', 'markets', 'on', 'in', 'by', 'in', 'in', 'in', 'on', 'in', 'in', 'of', 'for', 'will', 'for', 'in', 'in', 'in', 'on', 'for', 'of', ',', 'tuesday', 'in', 'in', 'early', 'on', 'on', 'of', 'attacked', 'of', 'on', 'reviewing', 'on', 'for', 'in', 'for', 'on', 'in', 'on', 'for', 'in', 'in', 'on', 'on', 'of', 'near', 'sensor', 'in', 'on', 'in', 'searching', '-', 'in', 'on', 'in', 'on', 'of', 'at', 'of', 'for', 'at', 'for', 'on', 'of', 'on', 'of', ',', 'of', 'of', 'in', 'on', 'of', 'with', 'of', 'in', 'of', 'on', 'of', 'of', 'of', 'as', 'in', 'at', 'of', 'in', 'for', 'for', 'of', 'of', 'here', 'in', 'in', 'email', 'in', '-', 'on', 'agency', 'of', 'on', 'of', 'this', 'for', 'and', 'on', 'at', 'of', 'in', 'of', 'for', 'for', 'in', 'in', 'in', 'in', 'of', 'of', 'on', 'of', 'on', 'in', 'in', 'of', '-', 'or', 'of', 'of', 'tuesday', 'for', 'of', 'in', 'of', 'of', 'at', 'at', 'donald', 'at', 'of', 'for', 'in', 'in', 'of', 'of', 'in', 'of', '.', 'for', 'of', 'in', 'at', 'and', 'on', 'of', 'of', 'at', 'on', 'of', 'of', 'for', 'of', ',', 'of', 'rover', 'of', 'conducted', 'on', 'for', 'of', 'in', 'at', 'on', 'in', 'of', 'of', 'of', 'for', 'of', 'in', 'drive', 'of', 'in', 'of', 'of', 'in', 'field', 'in', 'system', 'of', 'of', 'in', 'in', 'at', 'in', 'of', 'in', 'on', 'of', 'of', 'on', 'of', 'in', 'to', 'of', 'on', 'for', 'on', 'of', 'in', 'at', 'in', 'on', 'in', 'on', 'wednesday', 'at', '.', 'on', 'in', 'in', 'for', 'in', 'in', 'sector', 'of', 'for', 'of', 'for', ',', 'in', 'at', 'in', 'in', 'on', 'of', 'on', 'ltd', 'for', \"'\", 'of', 'at', 'in', '.', 'in', 'hosts', 'on', 'in', 'in', 'for', 'of', 'on', 'in', 'of', 'in', 'of', 'on', 'in', 'of', 'on', 'in', 'of', 'in', 'of', ',', 'on', 'of', 'on', 'with', 'room', 'in', 'on', 'in', 'of', 'for', \"'\", 'for', 'against', ',', ',', 'for', 'in', 'in', 'in', 'at', 'of', '-', 'in', 'resort', 'of', 'on', ',', 'in', 'on', 'of', 'on', 'on', 'on', 'at', 'despite', '.', 'of', 'for', 'of', 'on', 'for', 'of', 'that', 'on', 'for', 'president', 'in', 'of', 'on', 'on', 'in', 'trial', 'of', 'of', 'of', ',', 'of', 'for', 'of', 'of', 'on', 'on', \"''\", 'for', 'comes', 'on', 'to', 'on', 'in', 'in', 'of', 'in', 'who', 'of', 'for', 'in', 'of', 'of', 'in', 'in', 'for', 'in', 'of', 'of', 'for', 'in', 'in', 'at', 'here', 'of', 'on', 'of', 'in', 'of', 'on', 'of', 'due', 'for', 'of', 'in', 'of', 'on', 'in', 'in', 'in', 'of', ',', 'of', 'in', 'on', 'of', 'fatigue', 'on', 'on', 'in', 'of', 'on', 'thin', 'of', 'at', 'on', 'of', 'of', 'of', 'in', 'almost', 'in', 'for', 'in', 'on', 'of', 'on', 'on', 'of', 'on', 'of', 'bus', 'at', 'said', 'in', 'in', 'at', 'of', 'which', 'for', 'in', 'of', 'of', 'a', ',', 'early', 'in', 'for', 'on', 'for', 'of', 'in', 'on', 'in', '.', 'on', '.', 'on', 'of', 'in', 'of', ',', 'for', 'in', 'on', 'in', 'of', 'in', 'in', 'in', 'of', 'on', 'in', 'of', 'of', 'of', 'in', 'in', 'of', 'on', 'in', 'in', 'of', 'in', '.', 'on', 'on', 'in', 'in', 'of', 'in', 'of', 'on', 'in', 'on', 'of', 'of', 'in', 'in', 'reading', 'in', 'of', 'thursday', 'segments', 'of', 'in', 'in', 'in', 'in', 'of', 'most', 'of', 'on', 'early', 'in', 'of', 'on', 'of', 'in', 'in', 'in', 'for', 'for', 'of', 'from', 'at', 'of', 'at', 'in', 'of', 'for', 'of', 'in', 'wednesday', 'on', 'on', 'on', 'of', 'for', 'on', 'for', 'on', 'said', 'in', 'in', 'on', 'on', 'in', 'on', 'of', 'said', 'on', 'at', 'of', 'of', 'in', 'of', 'on', 'detonation', 'on', 'in', 'at', 'of', 'of', 'on', 'in', 'of', 'of', 'in', 'at', 'of', 'in', 'nation', 'on', 'and', 'of', 'at', 'of', 'on', 'in', 'on', 'of', 'of', 'of', '.', 'of', 'of', 'at', 'in', 'in', 'in', 'of', 'of', 'of', 'in', 'in', 'at', 'a', 'in', 'for', 'in', 'takes', 'in', 'on', 'at', 'on', 'of', 'of', 'on', 'in', 'in', 'to', 'in', 'on', 'in', 'of', \"''\", 'in', 'for', 'of', \"'\", 'in', 'of', 'in', 'of', 'on', 'in', 'of', '<UNK>', 'following', 'on', 'of', 'on', 'in', 'of', 'of', 'in', 'agreement', 'of', 'of', 'for', 'on', 'in', 'in', 'in', 'of', 'which', 'on', 'to', 'centred', '.', 'in', 'power', 'of', 'for', '<UNK>', 'in', 'of', 'at', 'in', 'in', 'at', 'in', 'of', 'of', 'on', 'for', 'commander', 'for', 'in', 'of', ',', 'the', 'in', 'of', 'to', 'in', 'on', 'in', 'to', 'in', 'on', 'how', 'of', 'in', 'and', 'in', 'of', 'and', 'on', 'in', 'in', 'in', 'in', 'for', 'on', 'for', 'in', 'on', 'company', 'on', 'for', 'of', 'of', 'in', 'of', 'for', 'in', 'in', 'on', 'for', 'in', 'of', 'of', 'tv', 'of', 'which', 'for', 'of', 'for', 'of', 'of', 'in', 'of', 'of', 'for', 'of', 'even', 'in', 'on', 'take', 'at', 'on', 'on', 'in', 'of', 'of', 'of', ',', 'to', 'of', 'of', 'on', 'in', 'of', 'at', 'on', 'to', 'in', 'on', 'five', 'for', 'living', 'of', 'on', 'of', 'of', 'for', 'in', 'in', ',', ',', 'on', 'of', 'in', 'in', 'in', 'after', 'out', 'over', 'for', 'at', '.', 'setting', 'in', 'of', '.', 'of', 'at', 'of', ',', 'in', 'chain', 'at', 'in', \"'\", 'in', 'ahead', 'in', 'at', 'be', 'can', 'of', 'in', 'showed', 'in', 'on', 'on', 'at', 'city', ',', 'at', 'on', 'at', 'of', 'and', 'of', 'on', 'of', 'for', 'in', 'a', 'of', 'of', 'at', 'of', 'in', 'in', 'for', 'of', 'of', 'for', 'on', 'of', 'will', 'their', 'of', 'source', 'in', 'on', 'of', 'for', 'at', 'on', 'of', 'for', 'collaborating', 'on', 'wednesday', 'of', 'after', '-', 'of', 'in', 'in', 'in', 'of', 'to', 'in', 'of', 'in', 'of', 'groups', 'of', 'of', 'on', 'in', 'of', 'on', 'of', 'of', 'on', 'at', 'for', 'on', 'after', 'of', 'of', 'of', 'of', 'to', 'for', 'of', 'from', 'in', 'on', 'at', 'in', 'of', 'for', 'of', 'in', 'in', 'of', '<UNK>', 'of', 'and', 'of', 'in', 'for', 'in', 'on', 'of', 'of', '.', 'in', 'of', 'of', 'in', 'of', 'were', 'on', 'in', 'of', 'wednesday', 'after', 'of', 'on', 'on', 'of', 'in', 'of', 'of', 'and', 'when', ',', 'of', 'in', 'of', 'on', 'of', 'of', 'programs', 'to', 'of', 'canberra', 'of', 'in', 'of', 'in', 'of', 'in', '.', 'of', 'by', 'in', 'of', 'to', 'in', 'on', 'following', 'a', 'for', 'over', 'of', 'in', 'of', 'has', 'of', 'of', 'at', 'components', 'in', 'next', 'in', 'of', 'or', 'to', 'on', 'on', 'of', 'on', 'in', 'for', 'of', 'in', 'of', 'in', 'of', 'of', 'on', 'of', 'of', 'in', 'for', 'strongly', 'on', 'of', 'of', 'for', 'of', ',', 'in', 'of', 'by', 'on', 'on', 'of', 'of', 'of', 'older', 'in', 'one', 'on', 'over', 'in', 'of', 'in', 'of', 'at', 'on', 'of', 'of', 'of', 'to', 'of', 'older', 'for', 'of', 'on', 'at', 'sunday', 'on', 'on', 'for', 'and', 'updates', 'on', 'of', 'of', 'to', 'upside', 'of', 'of', 'thing', 'in', 'on', 'of', 'of', 'of', 'for', 'of', 'for', 'on', 'in', 'of', 'of', 'for', 'in', '.', 'for', 'in', 'on', 'for', 'and', 'updates', 'on', 'of', 'of', 'on', 'of', 'on', 'in', 'in', 'for', 'after', 'for', 'on', 'on', 'of', 'of', 'on', 'in', 'on', 'of', 'in', 'nations', 'on', 'attack', 'in', 'of', 'in', 'of', 'of', 'in', 'in', 'in', 'on', 'for', 'of', 'on', 'of', 'on', 'of', 'friday', 'of', 'of', 'in', 'in', 'moscow', 'of', 'winners', 'here', 'of', ',', 'on', 'for', 'of', ',', 'on', 'for', 'at', 'of', 'in', 'in', 'of', 'for', 'in', 'of', 'of', 'in', '.', 'for', 'in', 'at', 'for', 'for', 'talks', 'of', 'for', 'of', 'in', 'in', 'in', 'in', 'at', 'of', 'on', 'in', 'on', 'in', 'gathered', 'on', 'of', 'in', 'in', 'of', 'older', 'of', 'in', 'of', 'of', 'of', '.', 'of', 'of', 'in', 'ben', 'of', 'in', 'on', 'on', 'of', 'friday', 'ben', 'of', 'at', 'in', 'forward', 'al', 'on', 'of', 'on', 'of', 'in', 'in', 'at', 'in', 'of', 'at', 'in', 'on', 'of', 'for', 'in', 'of', 'for', 'of', 'of', 'morning', 'in', 'for', 'for', 'of', 'in', 'on', 'for', 'in', 'in', 'for', 'of', 'on', 'wall', 'of', 'of', ',', 'on', '.', 'of', 'of', 'in', 'on', 'in', 'for', 'in', \"'\", '-', 'for', 'month', 'of', 'for', 'of', 'in', 'of', 'at', 'by', 'of', 'at', 'for', 'in', 'in', 'of', 'of', 'weeks', 'tuesday', 'at', 'of', 'in', 'of', 'on', 'for', 'in', 'fleeing', 'in', 'and', 'at', 'year', 'of', ',', 'of', 'for', 'yasser', '.', 'for', 'for', 'at', 'on', 'for', 'of', 'into', 'tiles', 'in', 'of', 'in', 'on', 'in', 'of', 'of', 'of', '.', 'of', 'said', 'of', 'in', 'of', 'on', 'at', 'in', 'of', 'pro', 'on', '.', 'giant', 'of', 'for', 'on', 'of', 'of', 'of', 'in', 'for', ',', 'of', 'sunday', 'of', 'of', 'of', 'on', 'of', 'in', 'in', 'in', 'in', 'the', 'to', '.', 'for', 'on', 'of', 'that', 'in', 'of', 'has', 'officials', 'for', 'on', \"'\", 'on', 'in', 'removed', 'at', 'in', 'in', 'for', 'of', 'in', 'in', 'in', 'in', 'and', 'of', 'in', 'in', 'in', 'of', 'of', 'in', 'in', 'for', 'has', 'will', 'in', '.', 'of', 'on', 'of', 'in', 'of', 'in', 'in', 'in', 'of', 'and', 'of', 'in', 'in', 'in', 'the', 'of', 'on', 'of', 'movement', 'for', 'of', 'custody', 'in', '-', 'of', 'of', 'on', 'of', 'in', 'in', 'for', 'that', 'of', 'in', 'of', 'of', 'on', 'in', 'in', 'about', 'in', 'broadcast', 'of', 'of', 'in', 'in', 'in', 'in', 'cause', 'for', 'of', ',', 'of', 'on', 'and', 'and', 'of', 'a', 'in', 'as', 'on', 'in', 'of', 'on', 'in', 'in', 'in', 'for', 'in', 'on', 'on', 'of', 'for', 'of', 'in', 'for', 'in', 'on', 'of', 'in', 'of', 'of', 'in', 'of', 'who', 'to', 'in', 'in', 'said', 'on', 'on', ',', 'older', 'of', 'speaker', 'against', 'for', 'legend', ',', 'of', 'of', 'of', 'of', 'for', 'humans', 'for', 'in', 'more', 'of', '-', 'in', 'in', 'of', 'for', 'for', 'at', 'headache', 'of', 'of', 'in', 'of', 'for', 'in', 'of', 'at', 'gave', 'of', 'of', 'here', 'of', 'in', 'for', 'deserted', 'in', 'of', 'for', 'on', 'for', '.', 'of', 'of', 'in', 'for', 'in', 'in', 'in', 'of', 'on', 'on', 'of', 'on', 'at', 'on', 'in', 'on', 'in', 'in', 'of', 'of', 'for', 'on', 'on', 'demonstrated', 'in', 'in', 'of', 'of', 'in', 'on', 'of', 'in', 'her', 'in', 'on', 'in', 'on', 'away', 'of', 'with', 'for', 'of', 'in', 'in', 'of', 'in', 'on', 'of', 'on', 'of', 'in', 'on', 'in', 'in', 'of', 'who', 'of', \"'\", 'monday', 'on', 'away', 'of', 'of', 'on', 'of', 'alleging', 'of', 'in', 'nine', 'for', 'revolt', 'in', 'in', 'at', 'on', 'gunned', 'on', 'in', 'in', 'on', 'in', 'of', 'in', 'on', 'on', 'of', '.', '.', 'of', 'of', 'until', 'in', 'of', '.', 'on', 'of', 'of', 'on', 'in', 'in', 'on', 'session', 'in', 'in', 'of', 'in', 'of', 'in', 'in', 'on', 'on', 'of', 'in', 'at', ',', 'in', 'in', 'the', 'of', 'of', 'on', 'on', 'of', 'of', 'of', 'at', '-', 'on', 'of', 'hit', 'in', 'on', 'of', 'on', 'of', 'of', 'of', 'of', 'on', 'for', 'for', 'on', 'and', ',', 'of', 'in', 'in', ',', 'of', 'in', 'in', 'on', 'of', 'dismissed', 'for', 'on', 'of', 'regional', 'for', 'of', 'on', 'on', 'in', 'information', 'to', 'on', 'of', 'on', 'for', 'of', 'of', 'in', 'in', 'in', 'for', 'of', 'on', 'of', 'on', 'on', 'in', 'for', 'on', 'in', 'in', 'economy', 'in', 'in', 'of', 'on', 'on', 'of', 'in', 'in', 'for', 'aimed', 'of', 'on', 'who', 'victory', 'at', 'in', 'in', 'of', 'in', 'of', 'on', 'at', 'in', 'of', 'for', 'of', 'of', 'of', 'in', 'on', '.', 'crowned', 'of', 'on', 'in', 'of', 'in', 'on', 'of', 'on', 'of', 'of', 'for', 'of', 'of', 'in', 'in', 'for', 'of', 'in', 'of', 'on', 'in', 'in', 'in', 'of', 'on', 'in', 'bull', 'of', 'at', 'of', 'for', 'on', 'of', 'of', 'in', 'of', 'there', 'in', 'in', 'of', 'on', 'of', 'of', 'focus', 'characters', 'said', 'of', 'to', 'of', 'of', 'on', 'in', 'deby', 'in', 'of', 'at', 'in', 'truck', 'of', 'for', 'in', 'in', 'of', 'in', 'for', 'for', 'of', 'in', 'for', 'of', 'of', 'in', ',', 'that', 'for', 'for', 'on', 'in', 'for', 'of', 'of', 'of', 'for', 'in', 'of', 'for', 'on', 'of', 'of', 'systems', 'in', 'in', 'in', 'in', '.', 'of', 'who', 'at', 'glasses', 'for', 'at', 'of', 'from', 'of', 'india', 'of', 'of', 'on', 'of', 'in', 'questions', 'for', 'on', 'of', 'of', 'in', 'of', 'in', 'on', '.', 'in', 'of', 'at', 'of', 'on', 'for', 'of', 'said', 'by', 'for', 'in', 'in', 'of', 'for', 'of', 'in', 'for', 'in', 'of', 'that', 'vladimir', 'related', 'of', 'spiritual', 'at', 'at', 'for', 'on', 'of', 'for', 'the', 'on', 'in', 'in', 'of', 'in', 'where', 'for', 'of', 'in', '<UNK>', 'of', 'in', 'in', 'technology', 'of', 'tons', 'on', 'of', 'on', 'on', 'of', 'for', 'for', 'on', 'of', '<UNK>', 'in', 'on', 'of', ',', 'of', 'on', 'of', 'in', 'of', ',', 'wednesday', 'was', 'of', 'of', 'on', 'in', 'of', 'in', 'of', 'in', 'for', 'in', \"'\", 'for', 'of', 'center', 'of', 'of', 'for', 'on', 'of', 'on', 'for', 'involving', 'in', 'of', 'of', 'on', 'for', 'on', 'in', 'in', 'for', 'of', 'of', 'in', 'on', 'to', 'in', 'in', 'for', 'of', 'on', 'in', ',', 'on', 'in', ',', 'on', 'of', 'to', 'in', 'of', 'on', 'in', 'and', 'in', 'of', 'of', 'of', 'in', 'on', 'of', ',', 'of', 'at', 'from', 'of', 'for', 'in', 'in', 'on', 'for', 'of', 'on', 'court', 'of', 'in', 'for', 'on', 'at', 'on', 'allowed', 'on', 'at', 'in', 'by', 'of', 'of', 'of', 'are', 'and', 'of', 'of', 'and', 'for', 'on', \"'\", 'for', 'in', 'in', 'can', ',', 'in', 'will', 'asking', 'for', 'only', 'people', 'but', 'of', 'on', 'on', 'in', 'of', 'in', 'as', 'of', 'of', 'on', 'of', 'via', 'of', 'of', 'in', 'for', 'for', 'in', 'forward', 'on', 'of', 'in', 'of', 'on', 'who', 'of', 'in', 'on', 'chain', 'of', 'in', 'on', 'in', 'on', 'on', 'of', 'in', 'for', 'in', 'of', 'for', 'in', 'of', 'on', 'of', 'in', 'on', 'in', 'in', 'has', 'for', 'of', 'of', 'off', 'of', 'of', 'your', 'on', 'on', 'in', 'since', 'strike', 'in', 'on', 'out', ',', 'in', 'for', 'of', 'diagnosed', 'died', 'of', 'of', 'at', 'on', 'for', 'of', 'of', 'on', 'on', 'in', 'of', 'in', 'we', 'the', 'on', 'of', 'in', 'to', 'title', 'for', 'in', 'here', ',', 'of', 'for', 'on', 'for', 'thursday', 'in', 'at', 'in', 'of', 'of', 'in', 'of', 'at', 'of', 'of', 'on', 'at', 'at', 'who', 'of', 'conversations', 'at', 'of', 'at', 'for', 'on', 'of', 'of', '-', 'on', 'at', ',', 'on', 'alive', 'at', 'from', 'more', 'in', 'in', 'in', 'of', 'in', 'on', 'invasion', 'of', 'in', 'on', 'in', 'in', 'on', 'into', 'on', 'for', 'of', 'said', 'in', 'for', 'on', 'in', 'in', 'in', 'in', 'per', 'on', 'on', 'in', 'of', 'of', 'of', 'to', 'in', 'on', 'for', 'for', '--', 'she', 'thursday', 'in', 'on', 'of', \"'\", 'in', 'in', 'of', 'on', 'for', 'in', 'for', 'for', 'of', 'of', 'in', 'of', 'and', 'in', 'for', 'of', 'on', 'in', 'at', 'on', 'rrb', 'on', 'on', 'for', 'on', 'of', 'on', 'for', 'that', 'of', 'on', 'of', 'on', ',', 'will', 'at', 'of', 'three', 'in', 'on', 'bangladesh', 'of', 'in', 'on', 'of', 'in', 'in', 'for', 'lrb', 'on', 'on', 'monday', 'for', 'on', 'in', 'on', 'in', 'in', 'in', 'centred', 'of', 'of', 'in', 'of', 'in', 'in', 'of', '.', 'in', 'in', 'of', 'for', 'wednesday', 'for', 'of', 'to', 'in', 'on', 'of', 'of', 'of', 'in', 'on', 'for', 'of', 'to', 'propaganda', 'of', 'in', 'in', 'in', 'on', 'at', 'on', 'of', 'on', 'in', 'at', \"'\", 'on', 'collaboration', 'of', 'in', 'in', 'after', 'of', 'in', 'of', 'in', 'years', 'astronauts', 'of', '.', 'for', 'for', 'of', 'for', 'in', 'on', 'in', 'home', 'in', 'in', 'on', 'who', 'on', 'of', 'on', 'to', 'of', 'of', 'in', 'that', 'released', 'for', 'as', 'of', 'of', 'tentative', 'for', 'on', 'on', 'on', 'in', 'on', 'on', 'in', 'in', 'in', 'shot', 'of', 'in', 'on', 'on', 'of', 'of', 'in', 'of', 'of', 'in', 'of', 'of', 'of', 'of', 'on', 'of', 'for', 'of', 'of', 'on', 'of', 'in', 'in', 'with', 'radovan', 'on', 'of', 'of', 'area', 'in', 'on', 'on', 'on', 'of', 'of', 'of', 'in', 'on', 'of', 'on', 'on', 'in', 'in', 'of', 'of', 'of', 'in', 'for', 'on', 'in', ',', 'of', 'in', 'in', 'for', 'on', 'of', 'in', 'at', 'of', 'at', 'of', 'on', 'at', 'for', 'for', 'in', 'in', 'wounded', 'in', 'for', 'of', 'on', 'of', 'into', 'on', 'on', 'on', 'of', 'on', 'on', ',', 'on', 'in', 'of', 'here', '.', 'of', 'said', 'of', 'in', 'by', 'on', 'of', 'on', 'on', 'in', 'of', 'on', 'of', 'at', 'on', 'on', '.', 'at', 'of', 'on', 'in', 'in', 'for', 'on', 'on', 'for', 'for', 'about', 'of', 'at', 'of', 'of', 'of', 'of', 'on', 'in', 'in', 'at', 'at', 'in', 'on', 'in', 'at', 'in', 'unless', 'at', 'alive', 'at', 'for', 'for', 'of', 'wednesday', 'in', 'in', 'of', 'at', 'of', 'at', 'for', ',', 'of', 'in', 'on', 'of', 'on', 'in', 'for', 'said', 'of', 'in', 'on', 'for', 'of', 'on', 'thursday', 'in', 'in', 'of', 'their', '.', 'on', 'at', 'of', 'unless', 'at', 'as', \"'\", 'of', 'of', 'on', 'friday', 'at', 'on', 'as', 'in', 'in', 'in', 'for', 'in', 'percent', 'to', 'on', 'was', 'of', 'of', 'by', '.', 'in', 'on', 'for', 'in', '.', 'on', 'in', 'on', 'of', 'of', 'on', 'for', 'centred', 'in', 'of', 'of', 'of', 'on', 'on', 'in', 'to', 'in', 'of', 'emerging', 'for', 'on', 'in', 'of', 'on', 'of', 'of', 'to', 'of', 'on', 'in', 'of', 'in', 'of', 'of', 'of', 'on', 'beijing', '.', 'for', \"'\", 'on', ',', 'on', 'of', 'of', 'in', 'in', 'in', 'of', 'of', 'from', 'by', 'of', 'of', '<UNK>', 'in', 'on', 'on', 'here', 'in', 'in', 'in', 'in', 'in', 'of', 'in', 'for', 'of', 'in', 'of', 'of', 'of', 'in', 'of', 'in', 'said', 'of', 'at', 'of', 'in', 'of', 'in', 'for', 'on', 'of', 'for', 'for', 'on', 'for', 'of', 'within', 'in', 'of', ',', 'of', 'on', 'on', 'for', 'on', 'in', 'for', 'at', 'of', 'of', 'on', 'on', 'in', 'in', 'in', 'for', ',', 'are', 'pavel', 'in', 'in', 'of', \"'\", 'on', 'in', 'in', 'the', 'on', 'for', 'of', 'if', 'of', 'in', 'on', 'plane', 'in', 'of', 'for', 'in', 'of', 'of', 'in', 'for', 'in', ',', 'for', 'on', 'tool', 'on', 'of', 'in', 'manned', 'in', 'on', 'friday', 'on', 'in', '-', 'for', 'of', 'on', 'on', 'of', 'of', 'on', 'of', 'on', 'of', 'on', 'in', 'on', 'in', 'in', 'of', 'on', 'for', 'of', 'of', 'on', 'of', 'of', 'of', ',', \"'\", 'of', 'of', 'of', 'in', 'of', 'in', 'of', 'of', 'restriction', 'of', 'of', 'of', 'in', 'on', 'in', 'here', 'on', 'in', 'of', 'of', 'traveled', 'of', 'of', 'review', 'at', 'of', 'of', ',', 'in', 'of', '<UNK>', ',', 'of', 'of', 'for', 'on', ',', 'of', 'in', 'sinlaku', '.', 'to', 'of', '.', 'for', 'of', 'on']\n",
            "Accuracy of prediction over dev dataset is 0.4779102384291725\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "f = open(\"./dev.out\",\"r\")\n",
        "gt = f.read()\n",
        "gt = re.split(\" |\\n\",gt)\n",
        "gt = [g for g in gt if g != \"\"]\n",
        "# print(len(gt))\n",
        "print(\"Groundtruth:\",end=\" \")\n",
        "print(gt)\n",
        "f.close()\n",
        "\n",
        "f = open(\"./dd.out\",\"r\")\n",
        "p = f.read()\n",
        "p = re.split(\" |\\n\",p)\n",
        "\n",
        "p = [pp for pp in p if pp != \"\"]\n",
        "# print(len(p))\n",
        "print(\"Prediction:\",end=\" \")\n",
        "print(p)\n",
        "f.close()\n",
        "\n",
        "acc = (np.array(gt)==np.array(p)).mean()\n",
        "print(\"Accuracy of prediction over dev dataset is\",acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EaFaCJE827B",
        "outputId": "0da5335a-de24-47ee-d9d2-0dc8e4c497d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of selected Trigram Model over test dataset: 150.31106233064594\n"
          ]
        }
      ],
      "source": [
        "test  = PlaintextCorpusReader(\"./\",\"test.in\")\n",
        "sents = test.sents()\n",
        "\n",
        "tri_test,_  = preprocessing.padded_everygram_pipeline(3,sents)\n",
        "\n",
        "\n",
        "ppl = []\n",
        "for sent in tri_test:\n",
        "    ppl.append(tri_StupidBackoff_model.perplexity(list(sent)))\n",
        "# ppl_mean = np.mean(ppl)\n",
        "ppl_mean = gmean(ppl)\n",
        "print(f\"Perplexity of selected Trigram Model over test dataset:\",ppl_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UFD5jho827B",
        "outputId": "06fea587-1424-4b88-d5a5-23980b44aa11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in \n",
            "\n",
            "at in \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "of of seven \n",
            "\n",
            "a \n",
            "\n",
            "for on \n",
            "\n",
            "for in on \n",
            "\n",
            "in at of to \n",
            "\n",
            "on in \n",
            "\n",
            "of in \n",
            "\n",
            "on for \n",
            "\n",
            "on \n",
            "\n",
            "a in on \n",
            "\n",
            "in \n",
            "\n",
            "routine on \n",
            "\n",
            "in of in of of \n",
            "\n",
            "and \n",
            "\n",
            "on of \n",
            "\n",
            "of on to deal \n",
            "\n",
            "on of to \n",
            "\n",
            "for of in \n",
            "\n",
            "of by \n",
            "\n",
            "in are on of on \n",
            "\n",
            "of selling \n",
            "\n",
            "on its of in \n",
            "\n",
            "on \n",
            "\n",
            "tuesday for \n",
            "\n",
            "of already \n",
            "\n",
            "for of \n",
            "\n",
            "of to in \n",
            "\n",
            "on at to \n",
            "\n",
            "of \n",
            "\n",
            "of for \n",
            "\n",
            "not in \n",
            "\n",
            ". \n",
            "\n",
            "traveled of \n",
            "\n",
            "in of <UNK> join \n",
            "\n",
            "in \n",
            "\n",
            "by \n",
            "\n",
            "on . in \n",
            "\n",
            "of in \n",
            "\n",
            "in in for for \n",
            "\n",
            "for \n",
            "\n",
            "in in of \n",
            "\n",
            "driver for \n",
            "\n",
            "in in \n",
            "\n",
            "of in as in \n",
            "\n",
            "in in \n",
            "\n",
            ", \n",
            "\n",
            "on of on from \n",
            "\n",
            "of a \n",
            "\n",
            "for research in \n",
            "\n",
            "built \n",
            "\n",
            "after in \n",
            "\n",
            ", of \n",
            "\n",
            "for in for of of \n",
            "\n",
            ", of of \n",
            "\n",
            "after in \n",
            "\n",
            "for \n",
            "\n",
            "for of of \n",
            "\n",
            "in \n",
            "\n",
            "from high of in \n",
            "\n",
            "of of \n",
            "\n",
            "on \n",
            "\n",
            "in \n",
            "\n",
            "in in of \n",
            "\n",
            "in of \n",
            "\n",
            "in \n",
            "\n",
            "in in \n",
            "\n",
            "for on \n",
            "\n",
            "in in tonnes of \n",
            "\n",
            "of their for \n",
            "\n",
            "of for for \n",
            "\n",
            "on basis \n",
            "\n",
            "is \n",
            "\n",
            "for \n",
            "\n",
            "from on for of \n",
            "\n",
            "of \n",
            "\n",
            "of of \n",
            "\n",
            "of of had \n",
            "\n",
            "of of \n",
            "\n",
            "on on of \n",
            "\n",
            "urged of for to on \n",
            "\n",
            "in at of \n",
            "\n",
            "of music in \n",
            "\n",
            "general for in of in \n",
            "\n",
            "will in of \n",
            "\n",
            "on in \n",
            "\n",
            "on \n",
            "\n",
            ", \n",
            "\n",
            "from \n",
            "\n",
            "of \n",
            "\n",
            "of on for world \n",
            "\n",
            "of ideas \n",
            "\n",
            "switch of and \n",
            "\n",
            "in ceasefire for , in \n",
            "\n",
            "for ran in \n",
            "\n",
            "pelosi for caused \n",
            "\n",
            "on at \n",
            "\n",
            "rather \n",
            "\n",
            "in \n",
            "\n",
            "at in \n",
            "\n",
            "in of on \n",
            "\n",
            "for in of \n",
            "\n",
            "for \n",
            "\n",
            "a \n",
            "\n",
            "on at of , of \n",
            "\n",
            "on \n",
            "\n",
            "in of \n",
            "\n",
            "of of batch at \n",
            "\n",
            "on for \n",
            "\n",
            "on \n",
            "\n",
            "in \n",
            "\n",
            "in <UNK> \n",
            "\n",
            "in \n",
            "\n",
            "on \n",
            "\n",
            "wyze \n",
            "\n",
            "for of of on \n",
            "\n",
            "has \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "in in in \n",
            "\n",
            "of of \n",
            "\n",
            "on \n",
            "\n",
            "of in \n",
            "\n",
            "of monday \n",
            "\n",
            "of of \n",
            "\n",
            "award \n",
            "\n",
            "in based \n",
            "\n",
            "of \n",
            "\n",
            "in on of of of \n",
            "\n",
            "at \n",
            "\n",
            "<UNK> in \n",
            "\n",
            "in \n",
            "\n",
            "on \n",
            "\n",
            "saying of for \n",
            "\n",
            "for of than \n",
            "\n",
            "the \n",
            "\n",
            "in of in \n",
            "\n",
            "for in of on of \n",
            "\n",
            "in of \n",
            "\n",
            "in \n",
            "\n",
            "in general of \n",
            "\n",
            "'' in of of employees on \n",
            "\n",
            "at on \n",
            "\n",
            ", on \n",
            "\n",
            "seen in \n",
            "\n",
            "in of \n",
            "\n",
            "at for of of \n",
            "\n",
            "of of \n",
            "\n",
            "of \n",
            "\n",
            "in \n",
            "\n",
            "on , of \n",
            "\n",
            "on \n",
            "\n",
            "of for of \n",
            "\n",
            "on of \n",
            "\n",
            "on of \n",
            "\n",
            "on \n",
            "\n",
            "could \n",
            "\n",
            "of likely \n",
            "\n",
            "votes some \n",
            "\n",
            "in for of \n",
            "\n",
            "on for \n",
            "\n",
            "to \n",
            "\n",
            ". at of \n",
            "\n",
            "to at on \n",
            "\n",
            "of for \n",
            "\n",
            "of in in of rose \n",
            "\n",
            "for of \n",
            "\n",
            "in for in , united \n",
            "\n",
            "in at \n",
            "\n",
            "in \n",
            "\n",
            "for of of ronaldo \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "in \n",
            "\n",
            "on on on \n",
            "\n",
            "in of \n",
            "\n",
            "for on in area \n",
            "\n",
            "in close of \n",
            "\n",
            "in \n",
            "\n",
            "on of in \n",
            "\n",
            "from of on \n",
            "\n",
            "of of \n",
            "\n",
            "in of \n",
            "\n",
            "in of for in \n",
            "\n",
            "on of \n",
            "\n",
            "on in on \n",
            "\n",
            "on in \n",
            "\n",
            "on the \n",
            "\n",
            "in for \n",
            "\n",
            "and in because \n",
            "\n",
            "in \n",
            "\n",
            "for . of \n",
            "\n",
            "of \n",
            "\n",
            "of \n",
            "\n",
            ". in \n",
            "\n",
            "in \n",
            "\n",
            "for in at at \n",
            "\n",
            "in for \n",
            "\n",
            "on \n",
            "\n",
            "in of of \n",
            "\n",
            "in of \n",
            "\n",
            "for on \n",
            "\n",
            "in \n",
            "\n",
            "for \n",
            "\n",
            "in in for \n",
            "\n",
            "in \n",
            "\n",
            "in on \n",
            "\n",
            "'' on for of \n",
            "\n",
            "in \n",
            "\n",
            "in to of \n",
            "\n",
            "at \n",
            "\n",
            "for for on of of at \n",
            "\n",
            "of \n",
            "\n",
            "have \n",
            "\n",
            "in on in \n",
            "\n",
            "on \n",
            "\n",
            "on \n",
            "\n",
            "on \n",
            "\n",
            "on \n",
            "\n",
            "in in \n",
            "\n",
            "tesco in \n",
            "\n",
            "to of \n",
            "\n",
            "in of on of \n",
            "\n",
            "in of co \n",
            "\n",
            "of of in that \n",
            "\n",
            "for in \n",
            "\n",
            "in mikheil \n",
            "\n",
            "in in of on \n",
            "\n",
            "on of in for \n",
            "\n",
            "despite \n",
            "\n",
            "for \n",
            "\n",
            "in \n",
            "\n",
            "on of \n",
            "\n",
            "on of \n",
            "\n",
            "of of \n",
            "\n",
            "for in , \n",
            "\n",
            "for of \n",
            "\n",
            "in of in of \n",
            "\n",
            "in and in . in \n",
            "\n",
            "businesses \n",
            "\n",
            "send on in \n",
            "\n",
            "of aside in \n",
            "\n",
            "for at \n",
            "\n",
            "of of \n",
            "\n",
            "for in \n",
            "\n",
            "of of , \n",
            "\n",
            "in for on for \n",
            "\n",
            "for of for \n",
            "\n",
            "in , in \n",
            "\n",
            "'' in \n",
            "\n",
            "in for in living \n",
            "\n",
            "of in \n",
            "\n",
            "for \n",
            "\n",
            "in of of for of \n",
            "\n",
            "for \n",
            "\n",
            "for as of \n",
            "\n",
            "in of of for of \n",
            "\n",
            "that in \n",
            "\n",
            "' \n",
            "\n",
            "for \n",
            "\n",
            "of in \n",
            "\n",
            "for for in of of \n",
            "\n",
            "of for \n",
            "\n",
            "of for on \n",
            "\n",
            "in for of \n",
            "\n",
            "in of for on \n",
            "\n",
            "on on \n",
            "\n",
            ". of \n",
            "\n",
            "of of in on in \n",
            "\n",
            "for of \n",
            "\n",
            "on of \n",
            "\n",
            "at of of slapped of \n",
            "\n",
            "of of \n",
            "\n",
            "for \n",
            "\n",
            "driver for \n",
            "\n",
            "on the on for of the on \n",
            "\n",
            "for for of \n",
            "\n",
            "was on imposed at \n",
            "\n",
            "of monday \n",
            "\n",
            "assailed trying \n",
            "\n",
            "for \n",
            "\n",
            "of \n",
            "\n",
            "in \n",
            "\n",
            "one of \n",
            "\n",
            "in of \n",
            "\n",
            "in \n",
            "\n",
            "of \n",
            "\n",
            "of \n",
            "\n",
            "of on \n",
            "\n",
            "of \n",
            "\n",
            "that of \n",
            "\n",
            "in at \n",
            "\n",
            "in of at \n",
            "\n",
            "on \n",
            "\n",
            "on \n",
            "\n",
            "of in , \n",
            "\n",
            "of of of \n",
            "\n",
            "on \n",
            "\n",
            "in for \n",
            "\n",
            ", in in said of for \n",
            "\n",
            "in \n",
            "\n",
            "in for \n",
            "\n",
            "for during for in \n",
            "\n",
            "in on \n",
            "\n",
            "in of in of \n",
            "\n",
            "sounded \n",
            "\n",
            "of \n",
            "\n",
            "of '' \n",
            "\n",
            "of \n",
            "\n",
            "in <UNK> in on likely \n",
            "\n",
            ", in of in of of \n",
            "\n",
            "bonds of in and \n",
            "\n",
            "in on \n",
            "\n",
            "in \n",
            "\n",
            "on \n",
            "\n",
            "on on for \n",
            "\n",
            "at at in \n",
            "\n",
            "of on \n",
            "\n",
            "and in \n",
            "\n",
            "for on \n",
            "\n",
            "as on ' \n",
            "\n",
            "could \n",
            "\n",
            "of at \n",
            "\n",
            "in of \n",
            "\n",
            "in \n",
            "\n",
            "for \n",
            "\n",
            "on in of \n",
            "\n",
            "washington of in \n",
            "\n",
            "on in of in \n",
            "\n",
            "on \n",
            "\n",
            ". \n",
            "\n",
            "of on , \n",
            "\n",
            "for \n",
            "\n",
            "in on . protection of of \n",
            "\n",
            "of on \n",
            "\n",
            "of on \n",
            "\n",
            "handed of men \n",
            "\n",
            "in of <UNK> \n",
            "\n",
            "over of \n",
            "\n",
            "some said in \n",
            "\n",
            "three of \n",
            "\n",
            "in \n",
            "\n",
            "on of \n",
            "\n",
            "on in on \n",
            "\n",
            "at in for of has \n",
            "\n",
            "in \n",
            "\n",
            "of in in \n",
            "\n",
            "of at \n",
            "\n",
            "on of <UNK> bomb \n",
            "\n",
            "on \n",
            "\n",
            "for on of \n",
            "\n",
            "of in \n",
            "\n",
            "of of \n",
            "\n",
            "in \n",
            "\n",
            "for at \n",
            "\n",
            "' at \n",
            "\n",
            "for in for of \n",
            "\n",
            "on of \n",
            "\n",
            "on \n",
            "\n",
            "of \n",
            "\n",
            "of change of of \n",
            "\n",
            "on \n",
            "\n",
            "in of in \n",
            "\n",
            "in \n",
            "\n",
            "of in \n",
            "\n",
            "in \n",
            "\n",
            "in after setup \n",
            "\n",
            "in of \n",
            "\n",
            "<UNK> of \n",
            "\n",
            "in . \n",
            "\n"
          ]
        }
      ],
      "source": [
        "train = PlaintextCorpusReader(\"./\",\"test.in\")\n",
        "sents = train.sents()\n",
        "f = open(\"./3036198102.test.out\",\"w+\")\n",
        "\n",
        "\n",
        "for sent in sents:\n",
        "    for i in range(len(sent)):\n",
        "        if sent[i] == \"PREP\":\n",
        "            # print(sent[i-2],sent[i+2])\n",
        "            ans = tri_StupidBackoff_model.generate(text_seed=[sent[i-4],sent[i-3],sent[i-2]])\n",
        "            # ans = tri_StupidBackoff_model.generate(text_seed=[sent[i-3],sent[i-2]])\n",
        "            # ans = bi_StupidBackoff_model.generate(text_seed=[sent[i-3],sent[i-2]])\n",
        "            # ans = bi_StupidBackoff_model.generate(text_seed=sent[i-2])\n",
        "            num = 0\n",
        "            while ans not in [\"at\", \"in\", \"of\", \"for\", \"on\"] and num <100:\n",
        "                ans = tri_StupidBackoff_model.generate(text_seed=[sent[i-4],sent[i-3],sent[i-2]])\n",
        "                # ans = tri_StupidBackoff_model.generate(text_seed=[sent[i-3],sent[i-2]])\n",
        "                # ans = bi_StupidBackoff_model.generate(text_seed=[sent[i-3],sent[i-2]])\n",
        "                # ans = bi_StupidBackoff_model.generate(text_seed=sent[i-2])\n",
        "                num += 1\n",
        "            print(ans,end=\" \")\n",
        "            f.write(str(ans)+\" \")\n",
        "    print(\"\\n\")\n",
        "    f.write(\"\\n\")\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}